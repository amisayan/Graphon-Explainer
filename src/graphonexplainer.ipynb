{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "from utils import stat_graph, split_class_graphs, align_graphs\n",
    "from utils import two_graphons_mixup, universal_svd\n",
    "from graphon_estimator import universal_svd\n",
    "from models import GIN,GCN\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "logdir='/home/sayan/g-mixup/logs'\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s: - %(message)s', datefmt='%Y-%m-%d')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tensorboard_writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_x(dataset):\n",
    "    if dataset[0].x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max( max_degree, degs[-1].max().item() )\n",
    "            data.num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "\n",
    "        if max_degree < 2000:\n",
    "            # dataset.transform = T.OneHotDegree(max_degree)\n",
    "\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = F.one_hot(degs, num_classes=max_degree+1).to(torch.float)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = ( (degs - mean) / std ).view( -1, 1 )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_onehot_y(dataset):\n",
    "\n",
    "    y_set = set()\n",
    "    for data in dataset:\n",
    "        y_set.add(int(data.y))\n",
    "    num_classes = len(y_set)\n",
    "\n",
    "    for data in dataset:\n",
    "        data.y = F.one_hot(data.y, num_classes=num_classes).to(torch.float)[0]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def mixup_cross_entropy_loss(input, target, size_average=True):\n",
    "    \"\"\"Origin: https://github.com/moskomule/mixup.pytorch\n",
    "    in PyTorch's cross entropy, targets are expected to be labels\n",
    "    so to predict probabilities this loss is needed\n",
    "    suppose q is the target and p is the input\n",
    "    loss(p, q) = -\\sum_i q_i \\log p_i\n",
    "    \"\"\"\n",
    "    assert input.size() == target.size()\n",
    "    assert isinstance(input, Variable) and isinstance(target, Variable)\n",
    "    loss = - torch.sum(input * target)\n",
    "    return loss / input.size()[0] if size_average else loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    graph_all = 0\n",
    "    for data in train_loader:\n",
    "        # print( \"data.y\", data.y )\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        y = data.y.view(-1, num_classes)\n",
    "        loss = mixup_cross_entropy_loss(output, y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        graph_all += data.num_graphs\n",
    "        optimizer.step()\n",
    "    loss = loss_all / graph_all\n",
    "    return model, loss\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        y = data.y.view(-1, num_classes)\n",
    "        loss += mixup_cross_entropy_loss(output, y).item() * data.num_graphs\n",
    "        y = y.max(dim=1)[1]\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += data.num_graphs\n",
    "    acc = correct / total\n",
    "    loss = loss / total\n",
    "    return acc, loss\n",
    "\n",
    "originaldataset=TUDataset(root=\"/home/sayan/g-mixup/data\",name='REDDIT-BINARY')\n",
    "dataset=list(originaldataset)\n",
    "random.shuffle(dataset)\n",
    "for graph in dataset:\n",
    "        graph.y = graph.y.view(-1)\n",
    "\n",
    "dataset = prepare_dataset_onehot_y(dataset)\n",
    "dataset = prepare_dataset_x( dataset )\n",
    "train_nums = int(len(dataset) * 0.8)\n",
    "train_val_nums = int(len(dataset) * 0.9)\n",
    "\n",
    "train_dataset = dataset[:train_nums]\n",
    "val_dataset = dataset[train_nums:train_val_nums]\n",
    "test_dataset = dataset[train_val_nums:]\n",
    "batch_size=32\n",
    "learning_rate=0.01\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "num_features = dataset[0].x.shape[1]\n",
    "num_classes = dataset[0].y.shape[0] \n",
    "\n",
    "print(\"Num features\",num_features)\n",
    "print(\"num_classes\",num_classes)\n",
    "model = GCN(num_features=num_features, num_classes=num_classes, num_hidden=64).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 2000):\n",
    "    model, train_loss = train(model, train_loader)\n",
    "    train_acc = 0\n",
    "    val_acc, val_loss = test(model, val_loader)\n",
    "    test_acc, test_loss = test(model, test_loader)\n",
    "    scheduler.step()\n",
    "    print(\"Train loss\",train_loss, \"Epoch\",epoch)\n",
    "    print(\"Test Accuracy\",test_acc,\"Test_loss\",test_loss)\n",
    "    tensorboard_writer.add_scalar('Train Loss', train_loss, epoch)\n",
    "    tensorboard_writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "    tensorboard_writer.add_scalar('Test Loss', test_loss, epoch)\n",
    "    tensorboard_writer.add_scalar('Validation Accuracy', val_acc, epoch)\n",
    "    tensorboard_writer.add_scalar('Test Accuracy', test_acc, epoch)\n",
    "\n",
    "    logger.info('Epoch: {:03d}, Train Loss: {:.6f}, Val Loss: {:.6f}, Test Loss: {:.6f},  Val Acc: {: .6f}, Test Acc: {: .6f}'.format(\n",
    "        epoch, train_loss, val_loss, test_loss, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " save_path='/home/sayan/g-mixup/model/redditbinary.pth'\n",
    " torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = '/home/sayan/g-mixup/model/redditbinary.pth'\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model architecture\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2=list(originaldataset)\n",
    "random.shuffle(dataset2)\n",
    "# for graph in dataset2:\n",
    "#      graph.y = graph.y.view(-1)\n",
    "\n",
    "# dataset = prepare_dataset_onehot_y(dataset2)\n",
    "dataset2 = prepare_dataset_x( dataset2 )\n",
    "num_features = dataset2[0].x.shape[1]\n",
    "num_classes = dataset2[0].y.shape[0]\n",
    "print(num_features)\n",
    "print(num_classes)\n",
    "#explain_loader= DataLoader(dataset2[:30], batch_size=1, shuffle=True)\n",
    "newdataset=[]\n",
    "model.to('cpu')\n",
    "for data in dataset2: \n",
    "    #data=data.to(device)\n",
    "    output = model(data.x, data.edge_index,None)\n",
    "    #print(\"Output is\",output)\n",
    "    #print(output)\n",
    "    #output=output.to(\"cpu\")\n",
    "\n",
    "    pred = output.max(dim=1)[1]\n",
    "    #print(pred)\n",
    "    if (pred==0):\n",
    "        data.y=torch.zeros_like(data.y)\n",
    "        #newdataset.append(data)\n",
    "        \n",
    "    if (pred==1):\n",
    "        data.y=torch.ones_like(data.y)\n",
    "   \n",
    "    newdataset.append(data)\n",
    "    #print(\"pred is\",pred)\n",
    "    #y = data.y.view(-1, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newdataset[1].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_x(dataset):\n",
    "    if dataset[0].x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            print(\"Type of degs is \", type(degs))\n",
    "            print(\"Type of _max_deg, degs[-1].max().item() is \", type(max_degree), type(degs[-1].max().item()))\n",
    "            max_degree = max( max_degree, torch.tensor(degs[-1]).max())\n",
    "            print(\"Type of max degree is \", type(max_degree))\n",
    "            data.num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "\n",
    "        if max_degree < 2000:\n",
    "            # dataset.transform = T.OneHotDegree(max_degree)\n",
    "\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = F.one_hot(degs, num_classes=max_degree+1).to(torch.float)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = ( (degs - mean) / std ).view( -1, 1 )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classgraphs=split_class_graphs(newdataset)\n",
    "\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(newdataset)\n",
    "resolution = int(median_num_nodes)\n",
    "#print(\"resolution is\",resolution)\n",
    "graphons=[]\n",
    "for label,graphs in classgraphs:\n",
    "    #print(\"Label is\",label)\n",
    "    #print(\"graph is\",graphs[0])\n",
    "    align_graphs_list, normalized_node_degrees, max_num, min_num = align_graphs(\n",
    "                    graphs, padding=True, N=resolution)\n",
    "    #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "    graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "    #print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "    graphons.append((label, graphon))\n",
    "#two_graphons = random.sample(graphons, 2)\n",
    "print(graphons)\n",
    "two_graphons= [graphons[0] , graphons[1]]\n",
    "print(graphons[0][0], graphons[1][0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)\n",
    "# print(new_graph)\n",
    "# print(ng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([115.,  41.,  12.,  11.,   8.,   6.,   9.,   5.,   4.,   9.,   4.,   6.,\n",
      "          5.,  10.,   5.,   6.,   5.,   4.,   3.,   7.,   6.,   4.,   3.,   4.,\n",
      "          3.,   4.,   5.,   4.,   2.,   3.,   7.,   5.,   3.,   5.,   1.,   2.,\n",
      "          2.,   3.,   3.,   3.,   4.,   1.,   2.,   2.,   4.,   6.,   2.,   2.,\n",
      "          3.,   4.,   3.,   3.,   3.,   2.,   2.,   3.,   4.,   4.,   1.,   2.,\n",
      "          5.,   5.,   2.,   3.,   1.,   1.,   2.,   2.,   1.,   2.,   1.,   2.,\n",
      "          3.,   3.,   3.,   3.,   2.,   3.,   4.,   1.,   1.,   3.,   3.,   2.,\n",
      "          3.,   1.,   2.,   2.,   1.,   2.,   1.,   5.,   2.,   2.,   2.,   3.,\n",
      "          2.,   4.,   1.,   2.,   1.,   1.,   1.,   1.,   2.,   4.,   2.,   2.,\n",
      "          2.,   2.,   3.,   4.,   3.,   3.,   2.,   3.,   2.,   3.,   2.,   4.,\n",
      "          2.,   1.,   3.,   1.,   1.,   1.,   4.,   1.,   1.,   1.,   3.,   1.,\n",
      "          2.,   4.,   1.,   1.,   1.,   3.,   1.,   1.,   1.,   2.,   1.,   3.,\n",
      "          1.,   3.,   1.,   1.,   2.,   2.,   2.,   2.,   1.,   1.,   1.,   2.,\n",
      "          1.,   2.,   1.,   1.,   1.,   2.,   2.,   2.,   1.,   1.,   1.,   1.,\n",
      "          1.,   2.,   1.,   1.,   1.,   1.,   2.,   1.,   1.,   1.,   2.,   2.,\n",
      "          4.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   1.,   2.,   1.,\n",
      "          1.,   1.,   2.,   2.,   1.,   1.,   2.,   2.,   2.,   2.,   2.,   1.,\n",
      "          2.,   1.,   2.,   2.,   1.,   4.,   1.,   2.,   2.,   1.,   2.,   1.,\n",
      "          1.,   1.,   1.,   1.,   1.,   1.,   2.,   2.,   1.,   2.,   1.,   1.,\n",
      "          1.,   1.,   1.,   3.,   2.,   3.,   3.,   1.,   1.,   1.,   1.,   1.]) <class 'torch.Tensor'> <class 'float'>\n",
      "tensor([  1.,   1., 341.,   1.,   1.,   1.,   8.,   1.,   1.,   1.,   2.,   3.,\n",
      "          1.,  12.,   1.,   3.,   3.,   1.,   1.,   2.,   2.,   1.,   1.,   1.,\n",
      "          1.,   6.,   3.,   4.,   4.,   1.,   1.,   1.,   1.,   1.,   3.,   3.,\n",
      "          1.,   1.,   1.,   4.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   1.,\n",
      "          1.,   1.,   5.,   1.,   1.,   7.,  19.,   1.,   1.,   1.,   1.,   2.,\n",
      "          2.,   2.,   1.,   1.,   4.,   2.,   1.,   5.,   1.,   1.,   1.,   1.,\n",
      "          4.,   1.,   1.,   1.,  14.,   1.,   1.,   1.,   1.,   1.,   1.,   5.,\n",
      "          3.,   1.,   1.,   1.,   3.,   2.,   1.,   1.,   2.,   1.,   1.,   4.,\n",
      "          1.,   1.,   1.,   1.,   2.,   1.,   1.,   1.,   6.,   1.,   8.,   3.,\n",
      "          1.,   1.,   2.,   3.,   2.,   1.,   2.,   1.,   1.,   1.,   1.,   1.,\n",
      "          2.,   5.,   1.,   1.,   2.,   1.,   1.,   1.,   1.,   1.,   4.,   1.,\n",
      "          1.,   1.,   1.,   3.,   2.,   1.,   1.,   1.,   1.,   2.,   1.,   1.,\n",
      "          1.,   3.,   2.,   1.,   2.,   1.,   5.,   2.,   2.,   1.,   1.,   1.,\n",
      "          2.,   2.,   4.,   2.,   1.,   2.,   3.,   1.,   7.,   2.,   5.,   1.,\n",
      "          1.,   1.,   2.,   1.,   1.,   1.,   1.,   4.,   4.,   7.,   1.,   1.,\n",
      "          1.,   1.,   1.,  24.,   2.,   1.,   1.,   1.,   1.,   4.,   1.,   2.,\n",
      "          1.,   1.,   1.,   1.,   5.,   1.,   1.,   1.,   2.,   2.,   1.,   1.,\n",
      "          1.,   1.,   3.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "          1.,   1.,   1.,   1.,  10.,   1.,   1.,   1.,   1.,   2.,   1.,   2.,\n",
      "          1.,   2.,   1.,   1.,  15.,   1.,   3.,   1.,   1.,   1.,   1.,   1.,\n",
      "          1.,   1.,   1.,   2.,   2.,   1.,   1.,   4.,   3.,   1.,   5.,   1.,\n",
      "          3.,   1.,   1.,   1.,   3.,   1.,   6.,   1.,   1.,   1.,   1.,   1.,\n",
      "          1.,   3.,   1.,   1.,   1.,   1.,   1.,   2.,   1.,   3.,   1.,   1.,\n",
      "          1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   1.,   1.,\n",
      "          3.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,  21.,   1.,   1.,   1.,\n",
      "          1.,   1.,   2.,   1.,   6.,   2.,   1.,   5.,   1.,   1.,   1.,   6.,\n",
      "          1.,   2.,   4.,   1.,   1.,   1.,   1.,   2.,   1.,   1.,   1.,   1.,\n",
      "          1.,   1.,   1.,   1.,   2.,   1.,   1.,   2.,   1.,   1.,   1.,   1.,\n",
      "          4.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,  11.,   1.,\n",
      "          1.,   2.,   1.,   1.,   3.,   1.,   1.,   2.,   1.,   2.,   1.,   2.,\n",
      "          1.,   1.,   1.,   1.,   1.,   1.,   2.,   1.,   1.,   1.,   1.,   2.,\n",
      "          1.,   1.,   1.,   1.,   4.,   1.,   1.,   1.,   1.,   1.,   1.,   3.,\n",
      "          1.,   1.,   1.,   1.,   1.,   1.,   1.,   7.,   1.,   1.,   2.,   1.,\n",
      "          2.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   9.,   2.,   1.,\n",
      "          2.,   1.,   1.,   2.,   1.,   1.,   2.,   8.,   1.,   1.,   1.,   1.,\n",
      "          1.,   2.,   2.,   2.,   1.,   3.,   1.,   1.,   2.,   1.,   1.,   1.,\n",
      "          1.,   1.,   2.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "          1.,   1.,   1.,   1.,   1.,   1.,   5.,   4.,   3.,   1.,   1.,   1.,\n",
      "          1.,   1.,   1.,   1.,   1.,   4.,   3.,   1.,   7.,   1.,   1.,   1.,\n",
      "          1.,   3.,   1.,   2.,   1.,   7.,   1.,   1.,   2.,   1.,   1.,   1.,\n",
      "          1.,   1.,   1.,   1.,   1.,   1.,   3.,   1.,   4.,   1.,   1.,   2.,\n",
      "          1.,   1.,   1.,   6.,   3.,   1.,   1.,   1.,   2.,   1.,   1.,   2.,\n",
      "          1.,   9.,   2.,   2.,   1.,   1.,   1.,   2.,   1.,   1.,   2.,   1.,\n",
      "          1.,   2.,   1.,   1.,   1.,   6.,   1.,  15.,   1.,   1.,   1.,   1.,\n",
      "          1.,   6.,   1.,   3.,   1.,   2.,   1.,   1.,   3.,   5.,   1.,   2.,\n",
      "          1.,   1.,   1.,   5.,   1.,   1.,   1.,   1.,   1.,   9.,  10.,   3.,\n",
      "          2.,   1.,   1.,   1.,   1.,   1.,   2.,   1.,   4.,   1.,   2.,   1.,\n",
      "          2.,   1.,   1.,   1.,   1.,   1.,   2.,   1.,   1.,   1.,   1.,   1.,\n",
      "          1.,   1.,   1.,   3.,   1.,   2.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "          3.,   1.,   1.,   1.,   2.,   1.,   1.,   3.,   1.,   1.,   4.,   1.,\n",
      "          1.,   1.,   1.,   3.,   1.,   1.,   3.,   3.,   1.,   1.,   1.,   1.,\n",
      "          1.,   7.,   1.,   1.,   1.,   2.,   5.,   1.,   1.,   3.,   1.,   1.,\n",
      "          1.,   2.,   2.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "          2.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   1.,   1.,   1.,\n",
      "          1.,   1.,   1.,   1.,   1.,   1.,  17.,   1.,   1.,   2.,   1.,   1.,\n",
      "          2.,   2.,   3.,   2.,   3.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "          3.,   1.,   1.,   1.,   2.,   1.,   1.,   2.,   1.,   1.,   1.,   1.,\n",
      "          1.,   1.,   2.,   1.,   7.,   1.,   1.,   1.,   2.,   1.,   1.,   3.,\n",
      "          2.,   3.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,  13.,   5.,\n",
      "          3.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   1.]) <class 'torch.Tensor'> <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "edgeind=new_graph[1].edge_index[0]\n",
    "print(degree(edgeind) , type(degree(edgeind)), type(degree(edgeind).max().item()))\n",
    "edgeind2=dataset3[1].edge_index[0]\n",
    "print(degree(edgeind2), type(degree(edgeind2)), type(degree(edgeind2).max().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of new graph is tensor([0.])\n",
      "2010\n",
      "None\n",
      "Type of degs is  <class 'list'>\n",
      "Type of _max_deg, degs[-1].max().item() is  <class 'int'> <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3669605/3079972405.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_degree = max( max_degree, torch.tensor(degs[-1]).max())\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(newlist))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(newlist[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m---> 10\u001b[0m newlist\u001b[38;5;241m=\u001b[39m\u001b[43mprepare_dataset_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewlist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m explainergraph\u001b[38;5;241m=\u001b[39mnewlist[\u001b[38;5;28mlen\u001b[39m(dataset3):]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(explainergraph[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx)\n",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m, in \u001b[0;36mprepare_dataset_x\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of degs is \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(degs))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of _max_deg, degs[-1].max().item() is \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(max_degree), \u001b[38;5;28mtype\u001b[39m(degs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[0;32m----> 9\u001b[0m max_degree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_degree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdegs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of max degree is \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(max_degree))\n\u001b[1;32m     11\u001b[0m data\u001b[38;5;241m.\u001b[39mnum_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m( torch\u001b[38;5;241m.\u001b[39mmax(data\u001b[38;5;241m.\u001b[39medge_index) ) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "#new_graph=[]\n",
    "\n",
    "new_graph = two_graphons_mixup(two_graphons, la=1.0, num_sample=10)\n",
    "print(\"Label of new graph is\",new_graph[0].y)\n",
    "dataset3=list(originaldataset)\n",
    "\n",
    "newlist=list(originaldataset)+new_graph\n",
    "print(len(newlist))\n",
    "print(newlist[-1].x)\n",
    "newlist=prepare_dataset_x(newlist)\n",
    "explainergraph=newlist[len(dataset3):]\n",
    "print(explainergraph[0].x)\n",
    "#print(new_graph[0].x,new_graph[0].y,new_graph[0].edge_index)\n",
    "max=0\n",
    "for data in explainergraph:\n",
    "    targetoutput=model(data.x,data.edge_index,None)\n",
    "    print(targetoutput)\n",
    "    soft=torch.nn.Softmax(dim=1)\n",
    "    problitites=soft(targetoutput)\n",
    "\n",
    "    \n",
    "    targetpred = targetoutput.max(dim=1)[1]\n",
    "    print(\"Label of explainer graph is\",targetpred)\n",
    "#newlist=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(two_graphons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "graph=Data()\n",
    "label=np.array(23)\n",
    "sample_graph_label = torch.from_numpy(label).type(torch.float32)\n",
    "graph.y=sample_graph_label\n",
    "print(graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
