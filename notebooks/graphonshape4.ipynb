{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import torch_geometric as torch_geometric\n",
    "import math\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nxLabels\n",
    "# Star-0\n",
    "# Windmill -1\n",
    "# Barbell-2\n",
    "# Wheel-3\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv,GINConv\n",
    "from torch.distributions import Bernoulli,Categorical\n",
    "import matplotlib.cm as cmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features=1\n",
    "count1=0\n",
    "totalnode=0\n",
    "for numdata in range(1000):\n",
    "  range1=random.randint(3,9)\n",
    "  range2=random.randint(3,5)\n",
    "  m=[i for i in range(range1)]\n",
    "  n=[i for i in range(range2)]\n",
    "  Cycle = nx.windmill_graph(range1,range2)\n",
    "  #Cycle=nx.balanced_tree(range2,range1)\n",
    "  num_nodes=nx.number_of_nodes(Cycle)\n",
    "  totalnode+=num_nodes\n",
    "\n",
    "  y=np.ones(num_nodes)\n",
    "\n",
    "  #Cycle.add_nodes_from([i in range(0,100)])\n",
    "  #p=math.ceil(random.uniform(5,8))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  data=pyg_utils.from_networkx(Cycle)\n",
    "  count1+=np.count_nonzero(y)\n",
    "\n",
    "  #print(y)\n",
    "\n",
    "  data.y=1\n",
    "  x=torch.ones(num_nodes,1)\n",
    "  x=x.float()\n",
    "\n",
    "  #print(deg.shape)\n",
    "\n",
    "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
    "  data.x=x\n",
    "  dataset.append(data)\n",
    "nx.draw_networkx(Cycle, node_size=150, node_color='red',with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features=1\n",
    "count1=0\n",
    "totalnode=0\n",
    "for numdata in range(1000):\n",
    "  range1=random.randint(2,5)\n",
    "  range2=random.randint(6,30)\n",
    "  m=[i for i in range(range1)]\n",
    "  n=[i for i in range(range1,range2)]\n",
    "  Cycle = nx.star_graph(range2)\n",
    "  num_nodes=nx.number_of_nodes(Cycle)\n",
    "  totalnode+=num_nodes\n",
    "\n",
    "  y=np.ones(num_nodes)\n",
    "\n",
    "  #Cycle.add_nodes_from([i in range(0,100)])\n",
    "  #p=math.ceil(random.uniform(5,8))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  data=pyg_utils.from_networkx(Cycle)\n",
    "  count1+=np.count_nonzero(y)\n",
    "\n",
    "  #print(y)\n",
    "\n",
    "  data.y=0\n",
    "  x=torch.ones(num_nodes,1)\n",
    "  x=x.float()\n",
    "\n",
    "  #print(deg.shape)\n",
    "\n",
    "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
    "  data.x=x\n",
    "  dataset.append(data)\n",
    "nx.draw_networkx(Cycle, node_size=150, node_color='red',with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features=1\n",
    "count1=0\n",
    "totalnode=0\n",
    "for numdata in range(1000):\n",
    "  #num_nodes=random.randint(3,10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #Cycle.add_nodes_from([i in range(0,100)])\n",
    "  p=math.ceil(random.uniform(1,4))\n",
    "  #Cycle = nx.full_rary_tree(p,num_nodes)\n",
    "  range1=random.randint(2,4)\n",
    "  range2=random.randint(4,15)\n",
    "  m=[i for i in range(range1)]\n",
    "  n=[i for i in range(range1,range2)]\n",
    "  Cycle=nx.barbell_graph(range1,range2)\n",
    "  num_nodes=nx.number_of_nodes(Cycle)\n",
    "  totalnode+=num_nodes\n",
    "  y=np.ones(num_nodes)\n",
    "\n",
    "  #Cycle=nx.wheel_graph(num_nodes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  data=pyg_utils.from_networkx(Cycle)\n",
    "  count1+=np.count_nonzero(y)\n",
    "\n",
    "  #print(y)\n",
    "\n",
    "  data.y=2\n",
    "  x=torch.ones(num_nodes,1)\n",
    "  x=x.float()\n",
    "\n",
    "  #print(deg.shape)\n",
    "\n",
    "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
    "  data.x=x\n",
    "  dataset.append(data)\n",
    "nx.draw_networkx(Cycle, node_size=150, node_color='red',with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features=1\n",
    "count1=0\n",
    "totalnode=0\n",
    "for numdata in range(1000):\n",
    "  #num_nodes=random.randint(3,10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #Cycle.add_nodes_from([i in range(0,100)])\n",
    "  p=math.ceil(random.uniform(1,4))\n",
    "  #Cycle = nx.full_rary_tree(p,num_nodes)\n",
    "  range1=random.randint(2,4)\n",
    "  range2=random.randint(6,15)\n",
    "  m=[i for i in range(range1)]\n",
    "  n=[i for i in range(range1,range2)]\n",
    "  #Cycle=nx.grid_2d_graph(m,n)\n",
    "  Cycle=nx.wheel_graph(range2)\n",
    "  num_nodes=nx.number_of_nodes(Cycle)\n",
    "  totalnode+=num_nodes\n",
    "  y=np.ones(num_nodes)\n",
    "\n",
    "  #Cycle=nx.wheel_graph(num_nodes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  data=pyg_utils.from_networkx(Cycle)\n",
    "  count1+=np.count_nonzero(y)\n",
    "\n",
    "  #print(y)\n",
    "\n",
    "  data.y=3\n",
    "  x=torch.ones(num_nodes,1)\n",
    "  x=x.float()\n",
    "\n",
    "  #print(deg.shape)\n",
    "\n",
    "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
    "  data.x=x\n",
    "  dataset.append(data)\n",
    "nx.draw_networkx(Cycle, node_size=150, node_color='red',with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originaldataset=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " save_path='/model/4shapes.pt'\n",
    " torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for loading saved dataset\n",
    "\n",
    "\n",
    "# Define the path to the saved dataset on your local machine\n",
    "\n",
    "\n",
    "# Define the path to the saved dataset on your local machine\n",
    "load_path_dataset = '/datasets/4shapes.pt'\n",
    "\n",
    "# Load the saved dataset (list of tensors)\n",
    "loaded_dataset = torch.load(load_path_dataset)\n",
    "\n",
    "\n",
    "# Load the saved dataset (list of tensors)\n",
    "loaded_dataset = torch.load(load_path_dataset)\n",
    "dataset=loaded_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_dataset = dataset[:3500]\n",
    "test_dataset = dataset[3500:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data.x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.bn(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "class CombinedModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.encoder = GCNEncoder(hidden_channels)\n",
    "        self.classifier = LinearClassifier(input_dim=hidden_channels, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Get the embeddings from the encoder\n",
    "        embeddings = self.encoder(x, edge_index, batch)\n",
    "\n",
    "        # Get the logits from the classifier\n",
    "        logits = self.classifier(embeddings)\n",
    "\n",
    "        return embeddings, logits\n",
    "model=CombinedModel(hidden_channels=64,num_classes=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CombinedModel(hidden_channels=64,num_classes=4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Add a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            embedding,  out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "            #print(out)\n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        # Update the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print the current learning rate every epoch (optional)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}\",loss)\n",
    "        # train_acc = test(train_loader)\n",
    "        # test_acc = test(test_loader)\n",
    "        # print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 400\n",
    "\n",
    "# Call the training loop\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_path = '/model/4shapes.pt'\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model architecture\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "classifieraccuracy=0\n",
    "for data in dataset[3500:]:\n",
    "    __,out=model(data.x, data.edge_index, data.batch)\n",
    "    if(data.y==out.argmax(dim=1)):\n",
    "        classifieraccuracy+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifieraccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(model, dataset, class_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided dataset, compute the confusion matrix,\n",
    "    and plot it with class names.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained GNN model\n",
    "    - dataset: List of data objects\n",
    "    - class_dict: Dictionary mapping class labels to class names, e.g., {0: 'Class A', 1: 'Class B'}\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Evaluate the model and get predictions and true labels\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            _, out = model(data.x, data.edge_index, data.batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_preds.append(pred)\n",
    "            all_labels.append(data.y)\n",
    "\n",
    "    all_preds = np.array(all_preds)#np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.array(all_labels)#np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Step 2: Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Step 3: Plot the confusion matrix\n",
    "    class_names = [class_dict[i] for i in range(len(class_dict))]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming the class labels are {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "#class_dict = {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "\n",
    "# Example dataset (assuming it's a list of data objects)\n",
    "# dataset = [...]\n",
    "\n",
    "# Call the function with the model, dataset (as a list), and class dictionary\n",
    "#plot_confusion_matrix(model, dataset, class_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict={0:'Star',1:'Windmill',2:'Barbell',3:'Wheel'}\n",
    "plot_confusion_matrix(model,dataset,class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifieraccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_path = '/model/4shapes.pth'\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model architecture\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=originaldataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=[]\n",
    "data2=[]\n",
    "data3=[]\n",
    "data4=[]\n",
    "\n",
    "latent_data1=[]\n",
    "latent_data2=[]\n",
    "latent_data3=[]\n",
    "latent_data4=[]\n",
    "i=0\n",
    "#print(data.batch)\n",
    "print(len(dataset))\n",
    "\n",
    "for data in dataset:\n",
    "    i=i+1\n",
    "    model.eval()\n",
    "    #data=dataset[i]\n",
    "    embedding , out = model(data.x, data.edge_index, data.batch)\n",
    "    pred = out.argmax(dim=1)\n",
    "    data.y=pred\n",
    "    #print(out)\n",
    "    if(pred==0):\n",
    "        data1.append(data)\n",
    "\n",
    "        latent_data1.append(embedding)\n",
    "    if(pred==1):\n",
    "        data2.append(data)\n",
    "        latent_data2.append(embedding)\n",
    "    if(pred==2):\n",
    "        data3.append(data)\n",
    "        latent_data3.append(embedding)\n",
    "    if(data.y==3):\n",
    "        data4.append(data)\n",
    "        latent_data4.append(embedding)\n",
    "\n",
    "\n",
    "print(len(data1))\n",
    "print(len(data2))\n",
    "print(len(data3))\n",
    "print(len(data4))\n",
    "print(\"i is\", i)\n",
    "\n",
    "#print(data.batch)\n",
    "\n",
    "#latent_explanations=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "from utils import stat_graph, split_class_graphs, align_graphs\n",
    "from utils import two_graphons_mixup, universal_svd\n",
    "from graphon_estimator import universal_svd\n",
    "from models import GIN,GCN\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "logdir=''\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s: - %(message)s', datefmt='%Y-%m-%d')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tensorboard_writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_x(dataset):\n",
    "    if dataset[0].x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max( max_degree, degs[-1].max().item() )\n",
    "            data.num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "\n",
    "        if max_degree < 2000:\n",
    "            # dataset.transform = T.OneHotDegree(max_degree)\n",
    "\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = F.one_hot(degs, num_classes=max_degree+1).to(torch.float)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = ( (degs - mean) / std ).view( -1, 1 )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_onehot_y(dataset):\n",
    "\n",
    "    y_set = set()\n",
    "    for data in dataset:\n",
    "        y_set.add(int(data.y))\n",
    "    num_classes = len(y_set)\n",
    "\n",
    "    for data in dataset:\n",
    "        data.y = F.one_hot(data.y, num_classes=num_classes).to(torch.float)[0]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def mixup_cross_entropy_loss(input, target, size_average=True):\n",
    "    \"\"\"Origin: https://github.com/moskomule/mixup.pytorch\n",
    "    in PyTorch's cross entropy, targets are expected to be labels\n",
    "    so to predict probabilities this loss is needed\n",
    "    suppose q is the target and p is the input\n",
    "    loss(p, q) = -\\sum_i q_i \\log p_i\n",
    "    \"\"\"\n",
    "    assert input.size() == target.size()\n",
    "    assert isinstance(input, Variable) and isinstance(target, Variable)\n",
    "    loss = - torch.sum(input * target)\n",
    "    return loss / input.size()[0] if size_average else loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for graph in dataset:\n",
    "        graph.y=torch.tensor(graph.y)\n",
    "        graph.y = graph.y.view(-1)\n",
    "\n",
    "dataset = prepare_dataset_onehot_y(dataset)\n",
    "#dataset = prepare_dataset_x( dataset )\n",
    "num_features = dataset[0].x.shape[1]\n",
    "num_classes = dataset[0].y.shape[0] \n",
    "\n",
    "print(\"Num features\",num_features)\n",
    "print(\"num_classes\",num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#explain_loader= DataLoader(dataset2[:30], batch_size=1, shuffle=True)\n",
    "newdataset=dataset\n",
    "\n",
    "classgraphs=split_class_graphs(newdataset)\n",
    "\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(newdataset)\n",
    "print(\"Average num nodes is\",avg_num_nodes)\n",
    "print(\"Average num edges is \",avg_num_edges)\n",
    "resolution = int(median_num_nodes)\n",
    "#print(\"resolution is\",resolution)\n",
    "graphons=[]\n",
    "for label,graphs in classgraphs:\n",
    "    #print(\"Label is\",label)\n",
    "    #print(\"graph is\",graphs[0])\n",
    "    align_graphs_list, normalized_node_degrees, max_num, min_num = align_graphs(\n",
    "                    graphs, padding=True, N=resolution)\n",
    "    #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "    graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "    #print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "    graphons.append((label, graphon))\n",
    "#two_graphons = random.sample(graphons, 2)\n",
    "class_graphons= [graphons[0] , graphons[1], graphons[2], graphons[3]]\n",
    "\n",
    "plt.figure(1)\n",
    "print(graphons[0][0])\n",
    "plt.axis('off')\n",
    "plt.imshow(graphons[0][1],cmap=\"inferno\")\n",
    "plt.figure(2)\n",
    "print(graphons[1][0])\n",
    "plt.imshow(graphons[1][1],cmap=\"inferno\")\n",
    "plt.axis('off')\n",
    "plt.figure(3)\n",
    "print(graphons[2][0])\n",
    "plt.imshow(graphons[2][1],cmap=\"inferno\")\n",
    "plt.axis('off')\n",
    "plt.figure(4)\n",
    "print(graphons[3][0])\n",
    "plt.imshow(graphons[3][1],cmap=\"inferno\")\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# print(new_graph)\n",
    "# print(ng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "# Star-0\n",
    "# Windmill -1\n",
    "# Barbell-2\n",
    "# Wheel-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "classlabel=graphons[3]\n",
    "two_graphons = [classlabel,classlabel]\n",
    "\n",
    "new_graph = two_graphons_mixup(two_graphons, la=1.0, num_sample=4)\n",
    "print(\"Label of new graph is\",new_graph[1].y)\n",
    "from torch_geometric.utils import to_networkx\n",
    "count=0\n",
    "for data in new_graph:\n",
    "    num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "    data.x= torch.ones(num_nodes,1)\n",
    "    embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "    soft=torch.nn.Softmax(dim=1)\n",
    "    problities=soft(out)\n",
    "    print(problities)\n",
    "    examplegraph=to_networkx(data,to_undirected=True)\n",
    "    examplegraph.remove_edges_from(nx.selfloop_edges(examplegraph))\n",
    "    plt.figure(count+1)\n",
    "    nx.draw_networkx(examplegraph, node_size=150, node_color='red',with_labels=False)\n",
    "    count+=1    \n",
    "print(\"Label of new graph is\",new_graph[1].y)\n",
    "endtime=time.time()\n",
    "executiontime=endtime-start_time\n",
    "print(\"executiontime is\",executiontime)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=model.classifier\n",
    "print(len(data1))\n",
    "print((len(data2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def adjacency_score(model,classifier, data_list1, data_list2, class_indices, num_samples=1000, steps=1000):\n",
    "    \"\"\"\n",
    "    Calculate adjacency score between two classes.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pre-trained model for classifying embeddings.\n",
    "        embeddings_list1 (list): List of embeddings for class 1.\n",
    "        embeddings_list2 (list): List of embeddings for class 2.\n",
    "        target_class_index (int): Index of the target class for adjacency check.\n",
    "        num_samples (int): Number of samples to draw from each list for checking.\n",
    "        steps (int): Number of steps for linear interpolation.\n",
    "\n",
    "    Returns:\n",
    "    return score      int: 1 if classes are adjacent, 0 otherwise.\n",
    "    \"\"\"\n",
    "    adjmeasure=0\n",
    "      # Assume classes are adjacent initially\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            # Randomly sample an embedding from each list\n",
    "            data1 = random.choice(data_list1)\n",
    "            data2 = random.choice(data_list2)\n",
    "            emb1,out1=model(data1.x,data1.edge_index,data1.batch)\n",
    "            emb2,out2=model(data2.x,data2.edge_index,data2.batch)\n",
    "            score=1\n",
    "            \n",
    "            # Convert to tensors if they're not already\n",
    "            if not isinstance(emb1, torch.Tensor):\n",
    "                emb1 = torch.tensor(emb1, dtype=torch.float32)\n",
    "            if not isinstance(emb2, torch.Tensor):\n",
    "                emb2 = torch.tensor(emb2, dtype=torch.float32)\n",
    "\n",
    "            for alpha in torch.linspace(0, 1, steps):\n",
    "                # Create a linear combination of the embeddings\n",
    "                combination = alpha * emb1 + (1 - alpha) * emb2\n",
    "\n",
    "                # Pass the combination through the model to get probabilities\n",
    "                output = classifier(combination)  # Add batch dimension\n",
    "                #print(output)\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "                #print(probabilities)\n",
    "\n",
    "                # Check if the maximum probability belongs to the target class\n",
    "                if probabilities.argmax(dim=1).item() not in class_indices:\n",
    "                    score = 0\n",
    "                    break;\n",
    "            adjmeasure+=score\n",
    "                  \n",
    "    return adjmeasure/num_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=model.classifier\n",
    "class_indices=[2,3]\n",
    "adjscore=adjacency_score(model,classifier,data3,data4,class_indices)\n",
    "print(adjscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graphons[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la= 0.5\n",
    "# classid1=class_indices[0]\n",
    "# classid2=class_indices[1]\n",
    "two_graphons=[graphons[3],graphons[1]]\n",
    "boundary_graph = two_graphons_mixup(two_graphons, la=la, num_sample=800,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la= 0.3\n",
    "class_indices=[2,3]\n",
    "classid1=class_indices[0]\n",
    "classid2=class_indices[1]\n",
    "two_graphons=[graphons[2],graphons[0]]\n",
    "accuracybound1=[]\n",
    "stdbound1=[]\n",
    "accuracybound2=[]\n",
    "stdbound2=[]\n",
    "lalist=[]\n",
    "boundaryembeddings=[]\n",
    "\n",
    "\n",
    "while(la<=0.8):\n",
    "    ratio= la/(1-la)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    boundaryaccuracy1=[]\n",
    "    boundaryaccuracy2=[]\n",
    "    for numexp in range(20):\n",
    "        boundary_graph = two_graphons_mixup(two_graphons, la=la, num_sample=800,show=True)\n",
    "        #print(\"Label of new graph is\",torch.argmax(new_graph[1].y,dim=-1))\n",
    "        label=torch.argmax(boundary_graph[1].y,dim=-1)\n",
    "        #print(label)\n",
    "    #from torch_geometric.utils import to_networkx\n",
    "        min=1\n",
    "        for data in boundary_graph:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            #print(problities)\n",
    "            if( abs(0.5-(problities[0][classid1]))+abs(0.5-problities[0][classid2])<min):#if( abs(problities[0][0]-0.5)<min):\n",
    "                #print(\"if\")\n",
    "                min= abs(0.5-(problities[0][classid1]))+abs(0.5-problities[0][classid2])#abs(problities[0][0]-0.5)\n",
    "                print(problities)\n",
    "                boundaryprobs1 = problities[0][classid1]\n",
    "                boundaryprobs2=problities[0][classid2]\n",
    "                latentboundary=embedding\n",
    "        boundaryaccuracy1.append(boundaryprobs1)\n",
    "        boundaryaccuracy2.append(boundaryprobs2)\n",
    "        boundaryembeddings.append(latentboundary)\n",
    "    boundaryaccuracy1=torch.stack(boundaryaccuracy1)\n",
    "    boundaryaccuracy2=torch.stack(boundaryaccuracy2)\n",
    "    accuracybound1.append(boundaryaccuracy1.mean(dim=0))\n",
    "    stdbound1.append(boundaryaccuracy1.std(dim=0))\n",
    "    accuracybound2.append(boundaryaccuracy2.mean(dim=0))\n",
    "    stdbound2.append(boundaryaccuracy2.std(dim=0))\n",
    "    lalist.append(la)\n",
    "    la=la+0.05\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latentclass=latent_data4\n",
    "margin=boundary_margin(latentclass[:len(boundaryembeddings)],boundaryembeddings)\n",
    "print(\"margin is\",margin)\n",
    "classifier=model.classifier\n",
    "\n",
    "thickness=boundary_thickness(latentclass[:len(boundaryembeddings)] ,boundaryembeddings,classifier,0,1)\n",
    "print(thickness)\n",
    "print(\"thickness is\",thickness)\n",
    "complexity=boundary_complexity(boundaryembeddings,64)\n",
    "print(\"complexity is\",complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing boundary metrics\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def boundary_margin(embeddings_c1, embeddings_c2):\n",
    "    \"\"\"\n",
    "    Compute the boundary margin.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings_c1 (torch.Tensor): Embeddings of class c1 graphs.\n",
    "    - embeddings_c2 (torch.Tensor): Embeddings of boundary graphs between class c1 and c2.\n",
    "    \n",
    "    Returns:\n",
    "    - margin (float): The boundary margin.\n",
    "\n",
    "    \"\"\"\n",
    "    embeddings_c1=torch.cat(embeddings_c1,dim=0)\n",
    "    embeddings_c2=torch.cat(embeddings_c2,dim=0)\n",
    "    distances = torch.norm(embeddings_c1 - embeddings_c2, dim=1)\n",
    "    margin = torch.min(distances).item()\n",
    "    return margin\n",
    "\n",
    "def boundary_thickness(embeddings_c1, embeddings_c1_c2, model, c1, c2, gamma=0.75, num_points=100):\n",
    "    thickness_values = []\n",
    "\n",
    "    for emb_c1, emb_c1_c2 in zip(embeddings_c1, embeddings_c1_c2):\n",
    "        t_values = torch.linspace(0, 1, num_points)\n",
    "        h_t = (1 - t_values).unsqueeze(1) * emb_c1 + t_values.unsqueeze(1) * emb_c1_c2\n",
    "        print(model(h_t).size())\n",
    "\n",
    "        # Compute the logits\n",
    "        logits_h_t = model(h_t)  # Assuming `model` is your classifier\n",
    "        probs_h_t = F.softmax(logits_h_t, dim=1)\n",
    "\n",
    "        # Compute the integrand\n",
    "        integrand = (gamma > (probs_h_t[:, c1] - probs_h_t[:, c2])).float()\n",
    "\n",
    "        # Approximate the integral using the trapezoidal rule\n",
    "        integral = torch.trapz(integrand, t_values)\n",
    "\n",
    "        # Compute the thickness value\n",
    "        thickness_value = (emb_c1 - emb_c1_c2).norm() * integral.mean()\n",
    "        thickness_values.append(thickness_value.item())\n",
    "\n",
    "    return sum(thickness_values) / len(thickness_values)\n",
    "\n",
    "# def boundary_complexity(embeddings, D):\n",
    "#     \"\"\"\n",
    "#     Compute the boundary complexity.\n",
    "    \n",
    "#     Args:\n",
    "#     - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
    "#     - D (int): Dimensionality of the embeddings.\n",
    "    \n",
    "#     Returns:\n",
    "#     - complexity (float): The boundary complexity.\n",
    "#     \"\"\"\n",
    "#     # Compute the covariance matrix of the embeddings\n",
    "#     embeddings=torch.cat(embeddings,dim=0)\n",
    "#     covariance_matrix = torch.cov(embeddings.T)\n",
    "    \n",
    "#     # Compute the eigenvalues of the covariance matrix\n",
    "#     eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
    "#     print(eigenvalues)\n",
    "    \n",
    "#     # Normalize the eigenvalues\n",
    "#     eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
    "#     print(eigenvalues_normalized)\n",
    "    \n",
    "#     # Compute the entropy of the normalized eigenvalues\n",
    "#     entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + 1e-7))\n",
    "#     print(entropy)\n",
    "    \n",
    "#     # Normalize the entropy by dividing it by log(D)\n",
    "#     complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
    "    \n",
    "#     return complexity.item()\n",
    "def boundary_complexity(embeddings, D, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Compute the boundary complexity.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
    "    - D (int): Dimensionality of the embeddings.\n",
    "    - epsilon (float): Small value added to eigenvalues to prevent log(0).\n",
    "    \n",
    "    Returns:\n",
    "    - complexity (float): The boundary complexity.\n",
    "    \"\"\"\n",
    "    # Flatten and concatenate embeddings\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    \n",
    "    # Compute the covariance matrix of the embeddings\n",
    "    covariance_matrix = torch.cov(embeddings.T)\n",
    "    \n",
    "    # Add a small value to the diagonal for regularization\n",
    "    covariance_matrix += epsilon * torch.eye(covariance_matrix.size(0))\n",
    "    \n",
    "    # Compute the eigenvalues of the covariance matrix\n",
    "    eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
    "    \n",
    "    # Clamp eigenvalues to avoid very small negative values due to numerical errors\n",
    "    eigenvalues = torch.clamp(eigenvalues, min=epsilon)\n",
    "    \n",
    "    # Normalize the eigenvalues\n",
    "    eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
    "    \n",
    "    # Compute the entropy of the normalized eigenvalues\n",
    "    entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + epsilon))\n",
    "    \n",
    "    # Normalize the entropy by dividing it by log(D)\n",
    "    complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
    "    \n",
    "    return complexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_graphons=[graphons[0],graphons[2]]\n",
    "boundary_graph=two_graphons_mixup(two_graphons,la=0.5,num_sample=1,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['red','blue']\n",
    "labels=['windmill','wheel']\n",
    "plot_two_means_with_error(accuracybound1,stdbound1,accuracybound2, stdbound2, lalist,labels,colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import two_graphons_mixup\n",
    "two_graphons = [graphons[3], graphons[0]]\n",
    "\n",
    "new_graph = two_graphons_mixup(two_graphons,la=0.3,num_sample=700)\n",
    "print(\"Label of new graph is\",new_graph[1].y)\n",
    "from torch_geometric.utils import to_networkx\n",
    "count=0\n",
    "chanceval=1\n",
    "for data in new_graph:\n",
    "    num_nodes = int( torch.max(data.edge_index) ) + 1# Example usage\n",
    "# Mean1 = np.random.rand(10)\n",
    "# Std1 = np.random.rand(10) * 0.1\n",
    "# Mean2 = np.random.rand(10)\n",
    "# Std2 = np.random.rand(10) * 0.1\n",
    "# Threshold = np.linspace(0, 1, 10)\n",
    "\n",
    "# plot_two_means_with_error(\n",
    "#     mean1=Mean1,\n",
    "#     std1=Std1,\n",
    "#     mean2=Mean2,\n",
    "#     std2=Std2,\n",
    "#     threshold=Threshold,\n",
    "#     labels=('Class 1', 'Class 2'),\n",
    "#     colors=('blue', 'green'),\n",
    "#     title='Mean with Error Bars for Two Classes'\n",
    "# )\n",
    "\n",
    "    data.x= torch.ones(num_nodes,1)\n",
    "    embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "    soft=torch.nn.Softmax(dim=1)\n",
    "    problities=soft(out)\n",
    "    #print(problities)\n",
    "    if(abs(0.5-(problities[0][classid1]))+abs(0.5-problities[0][classid2])<chanceval):\n",
    "        chanceval= abs(0.5-(problities[0][classid1]))+abs(0.5-problities[0][classid2])\n",
    "        bestdata=data\n",
    "        print(\"boundary probability is\",problities)\n",
    "        #print(\"chanceval is\",chanceval)\n",
    "\n",
    "examplegraph=to_networkx(bestdata,to_undirected=True)\n",
    "examplegraph.remove_edges_from(nx.selfloop_edges(examplegraph))\n",
    "plt.figure(23)\n",
    "nx.draw_networkx(examplegraph, node_size=150, node_color='red',with_labels=False)\n",
    "count+=1    \n",
    "print(\"Label of new graph should be\",new_graph[1].y)\n",
    "#print(\"Chance is\",chanceval)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_two_means_with_error(mean1, std1, mean2, std2, threshold, labels, colors, title=None,jitter=0.011):\n",
    "    \"\"\"\n",
    "    Plot two datasets with mean and error bars in the same figure.\n",
    "\n",
    "    Parameters:\n",
    "        mean1 (array_like): Array containing mean values for the first dataset.\n",
    "        std1 (array_like): Array containing standard deviation values for the first dataset.\n",
    "        mean2 (array_like): Array containing mean values for the second dataset.\n",
    "        std2 (array_like): Array containing standard deviation values for the second dataset.\n",
    "        threshold (array_like): Array containing threshold values.\n",
    "        labels (tuple): Tuple containing labels for the two datasets.\n",
    "        colors (tuple): Tuple containing colors for the two datasets.\n",
    "        title (str, optional): Title for the plot.\n",
    "    \"\"\"\n",
    "    # Flatten the arrays\n",
    "    mean1=torch.tensor(mean1,dtype=torch.float32)\n",
    "    std1=torch.tensor(std1,dtype=torch.float32)\n",
    "    mean2=torch.tensor(mean2,dtype=torch.float32)\n",
    "    std2=torch.tensor(std2,dtype=torch.float32)\n",
    "    mean1 = np.array(mean1).flatten()\n",
    "    std1 = np.array(std1).flatten()\n",
    "    mean2 = np.array(mean2).flatten()\n",
    "    std2 = np.array(std2).flatten()\n",
    "    threshold = np.array(threshold).flatten()\n",
    "        # Apply jitter\n",
    "    np.random.seed(0)  # Set seed for reproducibility\n",
    "    jitter1 = 0#np.random.normal(0, jitter, size=threshold.shape)\n",
    "    jitter2 = jitter#np.random.normal(0, jitter, size=threshold.shape)\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the first dataset\n",
    "    ax.errorbar(threshold+jitter1, mean1, yerr=std1, fmt='-', color=colors[0], label=labels[0])\n",
    "\n",
    "    # Plot the second dataset\n",
    "    ax.errorbar(threshold+jitter2, mean2, yerr=std2, fmt='-', color=colors[1], label=labels[1])\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel('Lambda')\n",
    "    ax.set_ylabel('Mean Class Score')\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # Show legend\n",
    "    ax.legend(loc='lower right', fontsize='small')\n",
    "\n",
    "    # Display plot\n",
    "    plt.show()\n",
    "\n",
    "# # Example usage\n",
    "# Mean1 = np.random.rand(10)\n",
    "# Std1 = np.random.rand(10) * 0.1\n",
    "# Mean2 = np.random.rand(10)\n",
    "# Std2 = np.random.rand(10) * 0.1\n",
    "# Threshold = np.linspace(0, 1, 10)\n",
    "\n",
    "# plot_two_means_with_error(\n",
    "#     mean1=Mean1,\n",
    "#     std1=Std1,\n",
    "#     mean2=Mean2,\n",
    "#     std2=Std2,\n",
    "#     threshold=Threshold,\n",
    "#     labels=('Class 1', 'Class 2'),\n",
    "#     colors=('blue', 'green'),\n",
    "#     title='Mean with Error Bars for Two Classes'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mean_with_error(mean, std, threshold,title=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plot mean with error bars.\n",
    "\n",
    "    Parameters:\n",
    "        mean (array_like): Array containing mean values.\n",
    "        std (array_like): Array containing standard deviation values.\n",
    "        threshold (array_like): Array containing threshold values.\n",
    "        label (str): Label for the data.ce(data_list1)\n",
    "            data2 = random.choice(data_list2)\n",
    "        color (str): Color of the line.\n",
    "        numsample (int): Sample number.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If not provided, a new figure will be created.\n",
    "    \"\"\"\n",
    "    # Flatten the arrays\n",
    "    mean=torch.tensor(mean,dtype=torch.float32)\n",
    "    std=torch.tensor(std,dtype=torch.float32)\n",
    "    mean = np.array(mean).flatten()\n",
    "    std = np.array(std).flatten()\n",
    "    threshold = np.array(threshold).flatten()\n",
    "    # # Select color automatically\n",
    "    # colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    # color = colors[numsample % 10]  # Cycle through colors\n",
    "\n",
    "    # Plotting\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.errorbar(threshold, mean, yerr=std, fmt='-')  # '-' for line\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel('Lambda')\n",
    "    ax.set_ylabel('Mean Class Score')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # ax.legend(loc='lower right',fontsize='small')  # Show legend\n",
    "    # ax.grid(True)  # Add grid\n",
    "# # Create a figure outside the function\n",
    "# fig, ax = plt.subplots()\n",
    "# plot_mean_with_error(Mean1,Std1,Threshold,label='class1',numsample=1,ax=ax)\n",
    "# plot_mean_with_error(Mean2,Std2,Threshold,label='class1',numsample=2,ax=ax)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explain_loader= DataLoader(dataset2[:30], batch_size=1, shuffle=True)\n",
    "newdataset=dataset\n",
    "\n",
    "classgraphs=split_class_graphs(newdataset)\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(newdataset)\n",
    "resolution = int(median_num_nodes)-7 # This parameter controls the number of nodes in the generated explanations\n",
    "mean_accuracy1=[]\n",
    "std_accuracy1=[]\n",
    "mean_accuracy2=[]\n",
    "std_accuracy2=[]\n",
    "mean_accuracy3=[]\n",
    "std_accuracy3=[]\n",
    "mean_accuracy4=[]\n",
    "std_accuracy4=[]\n",
    "ExplanationNodes=[]\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    #print(\"resolution is\",resolution)\n",
    "    graphons=[]\n",
    "    for label,graphs in classgraphs:\n",
    "        #print(\"Label is\",label)\n",
    "        #print(\"graph is\",graphs[0])\n",
    "        align_graphs_list, normalized_node_degrees, max_num, min_num = align_graphs(\n",
    "                        graphs, padding=True, N=resolution)\n",
    "        #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "        graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "        #print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "        graphons.append((label, graphon))\n",
    "    #two_graphons = random.sample(graphons, 2)\n",
    "    two_graphons1= [graphons[0] , graphons[1]]\n",
    "    two_graphons2=[graphons[2], graphons[3]]\n",
    "    explainer_graph1 = two_graphons_mixup(two_graphons1, la=0.0, num_sample=5)\n",
    "    explainer_graph2 = two_graphons_mixup(two_graphons1,la=1.0, num_sample=30)\n",
    "    explainer_graph3 = two_graphons_mixup(two_graphons2,la=0.0,num_sample=5)\n",
    "    explainer_graph4 = two_graphons_mixup(two_graphons2,la=1.0, num_sample=20)\n",
    "    label1=torch.argmax(explainer_graph1[0].y,dim=-1)\n",
    "    label2=torch.argmax(explainer_graph2[0].y,dim=-1)\n",
    "    label3=torch.argmax(explainer_graph3[0].y,dim=-1)\n",
    "    label4=torch.argmax(explainer_graph4[0].y,dim=-1)\n",
    "    accuracy1=[]\n",
    "    accuracy2=[]\n",
    "    accuracy3=[]\n",
    "    accuracy4=[]\n",
    "\n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "    for numexplanations in range(100):\n",
    "        max1=0\n",
    "        max2=0\n",
    "        max3=0\n",
    "        max4=0\n",
    "        for data in explainer_graph1:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            \n",
    "            problities=soft(out)\n",
    "            if(max1<problities[0][label1]):\n",
    "                max1= problities[0][label1]\n",
    "           \n",
    "        for data in explainer_graph2:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if (max2<problities[0][label2]):\n",
    "                max2= problities[0][label2]\n",
    "        for data in explainer_graph3:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if (max3<problities[0][label3]):\n",
    "                max3= problities[0][label3]\n",
    "        for data in explainer_graph4:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if (max4<problities[0][label4]):\n",
    "                max4= problities[0][label4]\n",
    "        accuracy1.append(max1)\n",
    "        accuracy2.append(max2)\n",
    "        accuracy3.append(max3)\n",
    "        accuracy4.append(max4)\n",
    "    accuracy1=torch.stack(accuracy1)\n",
    "    accuracy2=torch.stack(accuracy2)\n",
    "    accuracy3=torch.stack(accuracy3)\n",
    "    accuracy4=torch.stack(accuracy4)\n",
    "    mean1=accuracy1.mean(dim=0)\n",
    "    mean2=accuracy2.mean(dim=0)\n",
    "    mean3=accuracy3.mean(dim=0)\n",
    "    mean4=accuracy4.mean(dim=0)\n",
    "    std1=accuracy1.std(dim=0)\n",
    "    std2=accuracy2.std(dim=0)\n",
    "    std3=accuracy3.std(dim=0)\n",
    "    std4=accuracy4.std(dim=0)\n",
    "    mean_accuracy1.append(mean1)\n",
    "    mean_accuracy2.append(mean2)\n",
    "    mean_accuracy3.append(mean3)\n",
    "    mean_accuracy4.append(mean4)\n",
    "    std_accuracy1.append(std1)\n",
    "    std_accuracy2.append(std2)\n",
    "    std_accuracy3.append(std3)\n",
    "    std_accuracy4.append(std4)\n",
    "    ExplanationNodes.append(resolution)\n",
    "    resolution= resolution+1\n",
    "                \n",
    "            \n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_accuracy1)\n",
    "plot_mean_with_error(mean_accuracy1,std_accuracy1,ExplanationNodes)\n",
    "print(label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_accuracy2)\n",
    "plot_mean_with_error(mean_accuracy2,std_accuracy2,ExplanationNodes)\n",
    "print(label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_with_error(mean_accuracy3,std_accuracy3,ExplanationNodes)\n",
    "print(label3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_with_error(mean_accuracy4,std_accuracy4,ExplanationNodes)\n",
    "print(label4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
