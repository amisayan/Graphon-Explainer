{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import BA2MotifDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "from utils import stat_graph, split_class_graphs, align_graphs\n",
    "from utils import two_graphons_mixup, universal_svd\n",
    "from graphon_estimator import universal_svd\n",
    "from models import GIN,GCN\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "logdir=''\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s: - %(message)s', datefmt='%Y-%m-%d')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tensorboard_writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_x(dataset):\n",
    "    if dataset[0].x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max( max_degree, degs[-1].max().item() )\n",
    "            data.num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "\n",
    "        if max_degree < 2000:\n",
    "            # dataset.transform = T.OneHotDegree(max_degree)\n",
    "\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = F.one_hot(degs, num_classes=max_degree+1).to(torch.float)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = ( (degs - mean) / std ).view( -1, 1 )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_onehot_y(dataset):\n",
    "\n",
    "    y_set = set()\n",
    "    for data in dataset:\n",
    "        y_set.add(int(data.y))\n",
    "    num_classes = len(y_set)\n",
    "\n",
    "    for data in dataset:\n",
    "        data.y = F.one_hot(data.y, num_classes=num_classes).to(torch.float)[0]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def mixup_cross_entropy_loss(input, target, size_average=True):\n",
    "    \"\"\"Origin: https://github.com/moskomule/mixup.pytorch\n",
    "    in PyTorch's cross entropy, targets are expected to be labels\n",
    "    so to predict probabilities this loss is needed\n",
    "    suppose q is the target and p is the input\n",
    "    loss(p, q) = -\\sum_i q_i \\log p_i\n",
    "    \"\"\"\n",
    "    assert input.size() == target.size()\n",
    "    assert isinstance(input, Variable) and isinstance(target, Variable)\n",
    "    loss = - torch.sum(input * target)\n",
    "    return loss / input.size()[0] if size_average else loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    graph_all = 0\n",
    "    for data in train_loader:\n",
    "        # print( \"data.y\", data.y )\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        y = data.y.view(-1, num_classes)\n",
    "        loss = mixup_cross_entropy_loss(output, y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        graph_all += data.num_graphs\n",
    "        optimizer.step()\n",
    "    loss = loss_all / graph_all\n",
    "    return model, loss\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        _,output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        y = data.y.view(-1, num_classes)\n",
    "        loss += mixup_cross_entropy_loss(output, y).item() * data.num_graphs\n",
    "        y = y.max(dim=1)[1]\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += data.num_graphs\n",
    "    acc = correct / total\n",
    "    loss = loss / total\n",
    "    return acc, loss\n",
    "\n",
    "originaldataset=TUDataset(root=\"data\",name='MUTAG')\n",
    "dataset=list(originaldataset)\n",
    "random.shuffle(dataset)\n",
    "for graph in dataset:\n",
    "        graph.y = graph.y.view(-1)\n",
    "\n",
    "dataset = prepare_dataset_onehot_y(dataset)\n",
    "dataset = prepare_dataset_x( dataset )\n",
    "\n",
    "train_nums = int(len(dataset) * 0.8)\n",
    "train_val_nums = int(len(dataset) * 0.9)\n",
    "\n",
    "train_dataset = dataset[:train_nums]\n",
    "val_dataset = dataset[train_nums:train_val_nums]\n",
    "test_dataset = dataset[train_val_nums:]\n",
    "batch_size=32\n",
    "learning_rate=0.01\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "num_features = dataset[0].x.shape[1]\n",
    "num_classes = dataset[0].y.shape[0] \n",
    "\n",
    "print(\"Num features\",num_features)\n",
    "print(\"num_classes\",num_classes)\n",
    "model = GIN(num_features=num_features, num_classes=num_classes, num_hidden=64).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "def plotmutag(data):\n",
    "  cmap = colors.ListedColormap(['blue', 'black','red','yellow','orange','green','purple'])\n",
    "  ColorLegend = {'Carbon': 0,'Nitrogen': 1,'Oxygen': 2,'Fluorine': 3,'Iodine':4,'Chlorine':5,'Bromine':6}\n",
    "  cNorm  = colors.Normalize(vmin=0, vmax=6)\n",
    "  scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "  #print(cmap.colors)\n",
    "\n",
    "  exampledata=data\n",
    "  exfeatures=exampledata.x\n",
    "  #exlabel=exampledata.y\n",
    "  examplelabels=torch.argmax(exfeatures,dim=1)\n",
    "  #print(exlabel)\n",
    "  examplegraph=to_networkx(exampledata,to_undirected=True)\n",
    "  examplegraph.remove_edges_from(nx.selfloop_edges(examplegraph))\n",
    "  for component in list(nx.connected_components(examplegraph)):\n",
    "    if len(component)<7:\n",
    "        for node in component:\n",
    "            examplegraph.remove_node(node)\n",
    "  f = plt.figure(2,figsize=(8,8))\n",
    "  ax = f.add_subplot(1,1,1)\n",
    "  for label in ColorLegend:\n",
    "      ax.plot([0],[0],color=scalarMap.to_rgba(ColorLegend[label]),label=label)\n",
    "  nx.draw_networkx(examplegraph, node_size=150,node_color=examplelabels,cmap=cmap,vmin=0,vmax=6,with_labels=False,ax=ax)\n",
    "  plt.legend(fontsize=12,loc='best')\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_draw(self, graph):\n",
    "    attr = nx.get_node_attributes(graph, \"label\")\n",
    "    labels = {}\n",
    "    color = ''\n",
    "    for n in attr:\n",
    "        labels[n]= self.dict[attr[n]]\n",
    "        color = color+ self.color[attr[n]]\n",
    "        \n",
    "    #   labels=dict((n,) for n in attr)\n",
    "    nx.draw(graph,labels=labels, node_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=list(originaldataset)\n",
    "torch.manual_seed(12345)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_dataset = dataset[:150]\n",
    "test_dataset = dataset[:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data.x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(7, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.bn(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "class CombinedModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.encoder = GCNEncoder(hidden_channels)\n",
    "        self.classifier = LinearClassifier(input_dim=hidden_channels, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Get the embeddings from the encoder\n",
    "        embeddings = self.encoder(x, edge_index, batch)\n",
    "\n",
    "        # Get the logits from the classifier\n",
    "        logits = self.classifier(embeddings)\n",
    "\n",
    "        return embeddings, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn import JumpingKnowledge\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        # Adjust the number of input features (7) if necessary\n",
    "        self.conv1 = GCNConv(7, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels, heads=2, concat=False)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.residual = GCNConv(7, hidden_channels)\n",
    "        \n",
    "        # Update BatchNorm to handle concatenated features\n",
    "        self.bn = BatchNorm(hidden_channels * 3)\n",
    "        \n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(0.2)\n",
    "        self.jk = JumpingKnowledge(mode='cat')\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        residual = self.residual(x, edge_index)\n",
    "        \n",
    "        x1 = self.conv1(x, edge_index)\n",
    "        x1 = self.leaky_relu(x1)\n",
    "        \n",
    "        x2 = self.conv2(x1, edge_index)\n",
    "        x2 = self.leaky_relu(x2)\n",
    "\n",
    "        x3 = self.conv3(x2, edge_index) + residual\n",
    "\n",
    "        # Concatenate features from all layers\n",
    "        x = self.jk([x1, x2, x3])\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.bn(x)  # Normalize the concatenated features\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class CombinedModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.encoder = GCNEncoder(hidden_channels)\n",
    "        self.classifier = LinearClassifier(input_dim=hidden_channels * 3, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Get the embeddings from the encoder\n",
    "        embeddings = self.encoder(x, edge_index, batch)\n",
    "\n",
    "        # Get the logits from the classifier\n",
    "        logits = self.classifier(embeddings)\n",
    "\n",
    "        return embeddings, logits\n",
    "\n",
    "# Example usage\n",
    "# hidden_channels = 32  # Choose a suitable hidden size\n",
    "# num_classes = 2  # Number of classes in MUTAG\n",
    "\n",
    "# model = CombinedModel(hidden_channels=hidden_channels, num_classes=num_classes)\n",
    "\n",
    "# # Example input data (replace with actual data)\n",
    "# x = torch.rand((num_nodes, 7))  # Features\n",
    "# edge_index = torch.tensor([[0, 1], [1, 0]])  # Edge index (replace with actual edges)\n",
    "# batch = torch.tensor([0, 0])  # Batch tensor\n",
    "\n",
    "# output = model(x, edge_index, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CombinedModel(hidden_channels=64,num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Add a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(2000):\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            embedding,  out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "            #print(out)\n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        # Update the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print the current learning rate every epoch (optional)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}\",loss)\n",
    "        # train_acc = test(train_loader)\n",
    "        # test_acc = test(test_loader)\n",
    "        # print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 700\n",
    "\n",
    "# Call the training loop\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,dataset):\n",
    "    model.eval()\n",
    "    acc=0\n",
    "    for data in dataset:\n",
    "        emb,out=model(data.x,data.edge_index,data.batch)\n",
    "        if(out.argmax(dim=1)==data.y):\n",
    "            acc+=1\n",
    "    return acc/len(dataset)\n",
    "acc=test(model,dataset)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(model, dataset, class_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided dataset, compute the confusion matrix,\n",
    "    and plot it with class names.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained GNN model\n",
    "    - dataset: List of data objects\n",
    "    - class_dict: Dictionary mapping class labels to class names, e.g., {0: 'Class A', 1: 'Class B'}\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Evaluate the model and get predictions and true labels\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            _, out = model(data.x, data.edge_index, data.batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Step 2: Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Step 3: Plot the confusion matrix\n",
    "    class_names = [class_dict[i] for i in range(len(class_dict))]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming the class labels are {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "#class_dict = {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "\n",
    "# Example dataset (assuming it's a list of data objects)\n",
    "# dataset = [...]\n",
    "\n",
    "# Call the function with the model, dataset (as a list), and class dictionary\n",
    "#plot_confusion_matrix(model, dataset, class_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict={0:'Non-Mutagenic',1:'Mutagenic'}\n",
    "plot_confusion_matrix(model,dataset,class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 200):\n",
    "    model, train_loss = train(model, train_loader)\n",
    "    train_acc = 0\n",
    "    val_acc, val_loss = test(model, val_loader)\n",
    "    test_acc, test_loss = test(model, test_loader)\n",
    "    scheduler.step()\n",
    "    print(\"Train loss\",train_loss, \"Epoch\",epoch)\n",
    "    print(\"Test Accuracy\",test_acc,\"Test_loss\",test_loss)\n",
    "    tensorboard_writer.add_scalar('Train Loss', train_loss, epoch)\n",
    "    tensorboard_writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "    tensorboard_writer.add_scalar('Test Loss', test_loss, epoch)\n",
    "    tensorboard_writer.add_scalar('Validation Accuracy', val_acc, epoch)\n",
    "    tensorboard_writer.add_scalar('Test Accuracy', test_acc, epoch)\n",
    "\n",
    "    logger.info('Epoch: {:03d}, Train Loss: {:.6f}, Val Loss: {:.6f}, Test Loss: {:.6f},  Val Acc: {: .6f}, Test Acc: {: .6f}'.format(\n",
    "        epoch, train_loss, val_loss, test_loss, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc,_=test(model,test_loader)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " save_path='model/mutag.pth'\n",
    " torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = 'model/mutag.pth'\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model architecture\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataset2=list(originaldataset)\n",
    "classifieraccuracy=0\n",
    "random.shuffle(dataset2)\n",
    "# for graph in dataset2:\n",
    "#      graph.y = graph.y.view(-1)\n",
    "\n",
    "# dataset = prepare_dataset_onehot_y(dataset2)\n",
    "dataset2 = prepare_dataset_x( dataset2 )\n",
    "num_features = dataset2[0].x.shape[1]\n",
    "num_classes = dataset2[0].y.shape[0]\n",
    "print(num_features)\n",
    "print(num_classes)\n",
    "#explain_loader= DataLoader(dataset2[:30], batch_size=1, shuffle=True)\n",
    "newdataset=[]\n",
    "latentdata1=[]\n",
    "latentdata2=[]\n",
    "model.to('cpu')\n",
    "for data in dataset2: \n",
    "    #data=data.to(device)\n",
    "    emb,output = model(data.x, data.edge_index,data.batch)\n",
    "    #print(\"Output is\",output)\n",
    "    #print(output)\n",
    "    #output=output.to(\"cpu\")\n",
    "\n",
    "    pred = output.argmax(dim=1)\n",
    "    if(pred==data.y):\n",
    "        classifieraccuracy+=1\n",
    "    #print(pred)\n",
    "    if (pred==0):\n",
    "        data.y=torch.zeros_like(data.y)\n",
    "        latentdata1.append(emb)\n",
    "        #newdataset.append(data)\n",
    "        \n",
    "    if (pred==1):\n",
    "        data.y=torch.ones_like(data.y)\n",
    "        latentdata2.append(emb)\n",
    "   \n",
    "    newdataset.append(data)\n",
    "    #print(\"pred is\",pred)\n",
    "    #y = data.y.view(-1, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=newdataset\n",
    "classgraphs=split_class_graphs(dataset)\n",
    "\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(dataset)\n",
    "resolution = int(median_num_nodes)\n",
    "#print(\"resolution is\",resolution)\n",
    "graphons=[]\n",
    "for label,graphs in classgraphs:\n",
    "    #print(\"Label is\",label)\n",
    "    #print(\"graph is\",graphs[0])\n",
    "    align_graphs_list, normalized_node_degrees, max_num, min_num = align_graphs(\n",
    "                    graphs, padding=True, N=resolution)\n",
    "    #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "    graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "    #print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "    graphons.append((label, graphon))\n",
    "#two_graphons = random.sample(graphons, 2)\n",
    "print(graphons)\n",
    "two_graphons= [graphons[0] , graphons[1]]\n",
    "print(graphons[0][0], graphons[1][0])\n",
    "new_graph = two_graphons_mixup(two_graphons, la=1.0, num_sample=1,show=True)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)\n",
    "# print(new_graph)\n",
    "# print(ng)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the graphons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv,GINConv\n",
    "from torch.distributions import Bernoulli,Categorical\n",
    "import matplotlib.cm as cmxplt\n",
    "#print(graphons[1][1])\n",
    "maxval=0.028\n",
    "plt.figure(1)\n",
    "plt.axis('off')\n",
    "print(graphons[0][0])\n",
    "plt.imshow(graphons[0][1], cmap=\"inferno\")\n",
    "plt.figure(2)\n",
    "print(graphons[1][0])\n",
    "plt.axis('off')\n",
    "plt.imshow(graphons[1][1], cmap=\"inferno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=newdataset\n",
    "classgraphs=split_class_graphs(dataset)\n",
    "\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(dataset)\n",
    "resolution = int(median_num_nodes)\n",
    "#print(\"resolution is\",resolution)\n",
    "graphons=[]\n",
    "for label,graphs in classgraphs:\n",
    "    #print(\"Label is\",label)\n",
    "    #print(\"graph is\",graphs[0])\n",
    "    align_graphs_list, normalized_node_degrees, max_num, min_num = align_graphs(\n",
    "                    graphs, padding=True, N=resolution)\n",
    "    #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "    graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "    #print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "    graphons.append((label, graphon))\n",
    "#two_graphons = random.sample(graphons, 2)\n",
    "print(graphons)\n",
    "two_graphons= [graphons[0] , graphons[1]]\n",
    "print(graphons[0][0], graphons[1][0])\n",
    "new_graph = two_graphons_mixup(two_graphons, la=1.0, num_sample=1,show=True)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)\n",
    "# print(new_graph)\n",
    "# print(ng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import copy\n",
    "from typing import List, Tuple\n",
    "\n",
    "def align_x_graphscorrected2(\n",
    "    graphs: List[np.ndarray], \n",
    "    node_x: List[np.ndarray], \n",
    "    padding: bool = False, \n",
    "    N: int = None\n",
    ") -> Tuple[List[np.ndarray], np.ndarray, List[np.ndarray], int, int]:\n",
    "    \"\"\"\n",
    "    Align multiple graphs by sorting their nodes by descending node degrees\n",
    "    and perform max pooling over aligned node features.\n",
    "\n",
    "    :param graphs: A list of binary adjacency matrices\n",
    "    :param node_x: A list of node feature matrices (one-hot encoded)\n",
    "    :param padding: Whether to pad graphs to the same size\n",
    "    :param N: Target number of nodes for alignment (if specified)\n",
    "    :return:\n",
    "        aligned_graphs: A list of aligned adjacency matrices\n",
    "        final_node_features: The pooled node feature matrix\n",
    "        normalized_node_degrees: A list of sorted normalized node degrees (as node distributions)\n",
    "        max_num: Maximum number of nodes after alignment\n",
    "        min_num: Minimum number of nodes before alignment\n",
    "    \"\"\"\n",
    "    num_nodes = [graph.shape[0] for graph in graphs]\n",
    "    max_num = max(num_nodes)\n",
    "    min_num = min(num_nodes)\n",
    "\n",
    "    if N is None:\n",
    "        N = max_num  # Use the maximum number of nodes if N is not specified\n",
    "\n",
    "    aligned_graphs = []\n",
    "    aligned_node_features = []  # Accumulate node features\n",
    "    normalized_node_degrees = []\n",
    "    \n",
    "    for i in range(len(graphs)):\n",
    "        num_i = graphs[i].shape[0]\n",
    "\n",
    "        node_degree = 0.5 * np.sum(graphs[i], axis=0) + 0.5 * np.sum(graphs[i], axis=1)\n",
    "        node_degree /= np.sum(node_degree)\n",
    "\n",
    "        idx = np.argsort(node_degree)[::-1]  # Sort indices by descending node degree\n",
    "\n",
    "        sorted_node_degree = node_degree[idx].reshape(-1, 1)\n",
    "        sorted_graph = graphs[i][np.ix_(idx, idx)]\n",
    "\n",
    "        sorted_node_x = node_x[i][idx]\n",
    "\n",
    "        if padding:\n",
    "            normalized_node_degree = np.zeros((N, 1))\n",
    "            normalized_node_degree[:min(N, num_i), :] = sorted_node_degree[:min(N, num_i)]\n",
    "\n",
    "            aligned_graph = np.zeros((N, N))\n",
    "            aligned_graph[:min(N, num_i), :min(N, num_i)] = sorted_graph[:min(N, num_i), :min(N, num_i)]\n",
    "\n",
    "            aligned_node_x = np.zeros((N, node_x[i].shape[1]))\n",
    "            aligned_node_x[:min(N, num_i), :] = sorted_node_x[:min(N, num_i)]\n",
    "        else:\n",
    "            normalized_node_degree = sorted_node_degree[:N]\n",
    "            aligned_graph = sorted_graph[:N, :N]\n",
    "            aligned_node_x = sorted_node_x[:N]\n",
    "\n",
    "        aligned_graphs.append(aligned_graph)\n",
    "        aligned_node_features.append(aligned_node_x)  # Add aligned node features to the list\n",
    "        normalized_node_degrees.append(normalized_node_degree)\n",
    "\n",
    "    # Max pooling over the aligned node features\n",
    "    num_features = aligned_node_features[0].shape[1]\n",
    "    pooled_node_features = np.zeros((N, num_features))\n",
    "\n",
    "    for node_pos in range(N):\n",
    "        # Gather features at this node position across all graphs\n",
    "        features_at_pos = [\n",
    "            features[node_pos] \n",
    "            for features in aligned_node_features \n",
    "            if node_pos < features.shape[0]\n",
    "        ]\n",
    "        \n",
    "        # Count frequency of each feature\n",
    "        feature_counts = Counter(map(tuple, features_at_pos))\n",
    "\n",
    "        # Sort features by frequency, prioritizing non-zero features\n",
    "        most_common_features = sorted(\n",
    "            feature_counts.items(), \n",
    "            key=lambda item: (-item[1], item[0] == (0,) * num_features)\n",
    "        )\n",
    "\n",
    "        # Select the most common non-zero feature\n",
    "        for feature, _ in most_common_features:\n",
    "            if feature != (0,) * num_features:  # Ignore all-zero vector\n",
    "                pooled_node_features[node_pos] = feature\n",
    "                break\n",
    "\n",
    "    return aligned_graphs, pooled_node_features, normalized_node_degrees, N, min_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import stat_graph, split_class_x_graphs, align_x_graphs,align_x_graphscorrected\n",
    "from utils import two_x_graphons_mixup, universal_svd\n",
    "classgraphs=split_class_x_graphs(dataset)\n",
    "Alignedfeatures=[]\n",
    "#print(\"Classgraphs is\", classgraphs)\n",
    "\n",
    "\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(dataset)\n",
    "resolution = int(median_num_nodes)+6\n",
    "print(\"resolution is\",resolution)\n",
    "graphons=[]\n",
    "#print(classgraphs)\n",
    "for label,graphs,nodes in classgraphs:\n",
    "    print(len(graphs))\n",
    "    print(\"Label is\",label)\n",
    "\n",
    "    print(\"graph is\",np.shape(graphs[0]))\n",
    "\n",
    "    print(\"Nodes is\",np.shape(nodes[0]))\n",
    "\n",
    "\n",
    "\n",
    "    align_graphs_list,alignx, normalized_node_degrees, max_num, min_num = align_x_graphscorrected2(\n",
    "                    graphs,nodes, padding=True, N=resolution)\n",
    "    \n",
    "    #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "    print(\"Alignx is\",np.shape(alignx))\n",
    "\n",
    "    graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "    print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "    graphons.append((label, graphon))\n",
    "    Alignedfeatures.append(alignx)\n",
    "two_graphons= [graphons[0] , graphons[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Alignedfeatures[0])\n",
    "print(np.shape(Alignedfeatures))\n",
    "print(Alignedfeatures[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groundtruthset=list(originaldataset)\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "groundtruthset=list(originaldataset)\n",
    "random.shuffle(groundtruthset)\n",
    "#print(groundtruthset)\n",
    "count=0\n",
    "for i in range(len(groundtruthset)):\n",
    "    data=groundtruthset[i]\n",
    "    if(data.y==0):\n",
    "        #print(data.x)\n",
    "        class0graph=to_networkx(data,to_undirected=True)\n",
    "        class0graph.remove_edges_from(nx.selfloop_edges(class0graph))\n",
    "        count+=1\n",
    "        #plt.figure(1)\n",
    "        # plotmutag(data)\n",
    "        # break\n",
    "print(count)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_same_features_to_data(data_list, features_list):\n",
    "    \"\"\"\n",
    "    Assigns the same features to the x attribute of each Data object in data_list.\n",
    "\n",
    "    :param data_list: List of PyTorch Geometric Data objects\n",
    "    :param features_list: List of features to assign to the x attribute of each Data object\n",
    "    :return: List of Data objects with updated x attributes\n",
    "    \"\"\"\n",
    "    features_tensor = torch.tensor(features_list,dtype=torch.float)\n",
    "    \n",
    "    for data in data_list:\n",
    "        data.x = features_tensor\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plotmutag2(data):\n",
    "    cmap = colors.ListedColormap(['blue', 'black', 'red', 'yellow', 'orange', 'green', 'purple'])\n",
    "    ColorLegend = {\n",
    "        'Carbon': 0,\n",
    "        'Nitrogen': 1,\n",
    "        'Oxygen': 2,\n",
    "        'Fluorine': 3,\n",
    "        'Iodine': 4,\n",
    "        'Chlorine': 5,\n",
    "        'Bromine': 6\n",
    "    }\n",
    "    cNorm = colors.Normalize(vmin=0, vmax=6)\n",
    "    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "\n",
    "    # Extract node features and get the labels\n",
    "    exfeatures = data.x\n",
    "    examplelabels = torch.argmax(exfeatures, dim=1)\n",
    "\n",
    "    # Convert to NetworkX graph\n",
    "    examplegraph = to_networkx(data, to_undirected=True)\n",
    "    examplegraph.remove_edges_from(nx.selfloop_edges(examplegraph))\n",
    "\n",
    "    # Debug: Print initial number of nodes and labels\n",
    "    # print(f\"Initial number of nodes: {examplegraph.number_of_nodes()}\")\n",
    "    # print(f\"Initial number of labels: {len(examplelabels)}\")\n",
    "\n",
    "    # Remove small components and update labels\n",
    "    nodes_to_keep = set()\n",
    "    for component in list(nx.connected_components(examplegraph)):\n",
    "        if len(component) >= 7:\n",
    "            nodes_to_keep.update(component)\n",
    "    \n",
    "    examplegraph = examplegraph.subgraph(nodes_to_keep).copy()\n",
    "\n",
    "    # Filter examplelabels to only include nodes in the subgraph\n",
    "    nodes_list = list(examplegraph.nodes())\n",
    "    examplelabels = examplelabels[nodes_list]\n",
    "\n",
    "    # Debug: Print number of nodes and labels after filtering\n",
    "    # print(f\"Filtered number of nodes: {examplegraph.number_of_nodes()}\")\n",
    "    # print(f\"Filtered number of labels: {len(examplelabels)}\")\n",
    "\n",
    "    # Plot the graph\n",
    "    f = plt.figure(2, figsize=(8, 8))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "    for label in ColorLegend:\n",
    "        ax.plot([0], [0], color=scalarMap.to_rgba(ColorLegend[label]), label=label)\n",
    "    nx.draw_networkx(\n",
    "        examplegraph,\n",
    "        node_size=150,\n",
    "        node_color=examplelabels,\n",
    "        cmap=cmap,\n",
    "        vmin=0,\n",
    "        vmax=6,\n",
    "                # plotmutag(data)\n",
    "        # breakels=False,\n",
    "        ax=ax,with_labels=False\n",
    "    )\n",
    "    #plt.legend(fontsize=12, loc='best')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# plotmutag(data)  # Assuming 'data' is a PyTorch Geometric data object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_graph=[]\n",
    "import time\n",
    "start_time=time.time()\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "for y in range(1):\n",
    "    new_graph = two_graphons_mixup(two_graphons, la=0.0, num_sample=20)\n",
    "    print(\"Label of new graph is\",new_graph[0].y)\n",
    "    new_graph= assign_same_features_to_data(new_graph,Alignedfeatures[0])\n",
    "    # dataset3=list(originaldataset)\n",
    "\n",
    "    # newlist=list(originaldataset)+new_graph\n",
    "    # # print(len(newlist))\n",
    "    # # print(newlist[-1].x)\n",
    "    # newlist=prepare_dataset_x(newlist)\n",
    "    explainergraph=new_graph\n",
    "    count=0\n",
    "    # print(explainergraph[0].x)\n",
    "    #print(new_graph[0].x,new_graph[0].y,new_graph[0].edge_index)\n",
    "    #max=0\n",
    "    maxprob=0\n",
    "    for data in explainergraph:\n",
    "        _,targetoutput=model(data.x,data.edge_index,None)\n",
    "        #print(targetoutput)\n",
    "        soft=torch.nn.Softmax(dim=1)\n",
    "        problitites=soft(targetoutput)\n",
    "        targetpred = targetoutput.max(dim=1)[1]\n",
    "        \n",
    "        #print(\"Probabilities are\", problitites[0][0])\n",
    "        if (maxprob<problitites[0][1]):\n",
    "            maxprob= problitites[0][1]\n",
    "            print(problitites[0][1])\n",
    "            bestdata=data\n",
    "\n",
    "        \n",
    "\n",
    "            #print(\"Label of explainer graph is\",targetpred)\n",
    "\n",
    "    # examplegraph=to_networkx(bestdata,to_undirected=True)\n",
    "    # examplegraph.remove_edges_from(nx.selfloop_edges(examplegraph))\n",
    "    # for component in list(nx.connected_components(examplegraph)):\n",
    "    #     if len(component)<7:\n",
    "    #         for node in component:\n",
    "    #             examplegraph.remove_node(node)\n",
    "    # pos = nx.spring_layout(examplegraph, scale=20.0)\n",
    "    plt.figure(y+1)\n",
    "    #print(\"Bestdata is\",bestdata.x)\n",
    "    plotmutag2(bestdata)\n",
    "endtime=time.time()\n",
    "executiontime=endtime-start_time\n",
    "print(\"executiontime is\",executiontime)\n",
    "    #newlist=[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la= 0.3\n",
    "accuracybound=[]\n",
    "stdbound=[]\n",
    "lalist=[]\n",
    "boundaryembeddings=[]\n",
    "while(la<=0.7):\n",
    "    ratio= la/(1-la)\n",
    "\n",
    "    \n",
    "    boundary_graph = two_graphons_mixup(two_graphons, la=la, num_sample=100,show=True)\n",
    "    boundary_graph= assign_same_features_to_data(boundary_graph,Alignedfeatures[0])\n",
    "    #print(\"Label of new graph is\",torch.argmax(new_graph[1].y,dim=-1))\n",
    "    label=torch.argmax(boundary_graph[1].y,dim=-1)\n",
    "    print(label)\n",
    "    #from torch_geometric.utils import to_networkx\n",
    "    \n",
    "    boundaryaccuracy=[]\n",
    "    for numexp in range(20):\n",
    "        min1=1\n",
    "        for data in boundary_graph:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            #new_graph= assign_same_features_to_data(new_graph,Alignedfeatures[0])\n",
    "            #data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if( abs(problities[0][0]-0.5)<min1):\n",
    "                #print(\"if\")\n",
    "                min1= abs(problities[0][0]-0.5)\n",
    "                boundaryprobs = problities[0][0]\n",
    "                latentboundary=embedding\n",
    "                bestdata=data\n",
    "        #plotmutag2(bestdata)\n",
    "        boundaryaccuracy.append(boundaryprobs)\n",
    "        boundaryembeddings.append(latentboundary)\n",
    "    boundaryaccuracy=torch.stack(boundaryaccuracy)\n",
    "    accuracybound.append(boundaryaccuracy.mean(dim=0))\n",
    "    stdbound.append(boundaryaccuracy.std(dim=0))\n",
    "    lalist.append(la)\n",
    "    la=la+0.05\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latentdata1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latentclass=latentdata1\n",
    "margin=boundary_margin(latentclass,boundaryembeddings[:len(latentclass)])\n",
    "print(\"margin is\",margin)\n",
    "classifier=model.classifier\n",
    "\n",
    "thickness=boundary_thickness(latentclass ,boundaryembeddings[:len(latentclass)],classifier,0,1)\n",
    "print(thickness)\n",
    "print(\"thickness is\",thickness)\n",
    "complexity=boundary_complexity(boundaryembeddings,64)\n",
    "print(\"complexity is\",complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing boundary metrics\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def boundary_margin(embeddings_c1, embeddings_c2):\n",
    "    \"\"\"\n",
    "    Compute the boundary margin.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings_c1 (torch.Tensor): Embeddings of class c1 graphs.\n",
    "    - embeddings_c2 (torch.Tensor): Embeddings of boundary graphs between class c1 and c2.\n",
    "    \n",
    "    Returns:\n",
    "    - margin (float): The boundary margin.\n",
    "\n",
    "    \"\"\"\n",
    "    embeddings_c1=torch.cat(embeddings_c1,dim=0)\n",
    "    embeddings_c2=torch.cat(embeddings_c2,dim=0)\n",
    "    distances = torch.norm(embeddings_c1 - embeddings_c2, dim=1)\n",
    "    margin = torch.min(distances).item()\n",
    "    return margin\n",
    "\n",
    "def boundary_thickness(embeddings_c1, embeddings_c1_c2, model, c1, c2, gamma=0.75, num_points=100):\n",
    "    thickness_values = []\n",
    "\n",
    "    for emb_c1, emb_c1_c2 in zip(embeddings_c1, embeddings_c1_c2):\n",
    "        t_values = torch.linspace(0, 1, num_points)\n",
    "        h_t = (1 - t_values).unsqueeze(1) * emb_c1 + t_values.unsqueeze(1) * emb_c1_c2\n",
    "        print(model(h_t).size())\n",
    "\n",
    "        # Compute the logits\n",
    "        logits_h_t = model(h_t)  # Assuming `model` is your classifier\n",
    "        probs_h_t = F.softmax(logits_h_t, dim=1)\n",
    "\n",
    "        # Compute the integrand\n",
    "        integrand = (gamma > (probs_h_t[:, c1] - probs_h_t[:, c2])).float()\n",
    "\n",
    "        # Approximate the integral using the trapezoidal rule\n",
    "        integral = torch.trapz(integrand, t_values)\n",
    "\n",
    "        # Compute the thickness value\n",
    "        thickness_value = (emb_c1 - emb_c1_c2).norm() * integral.mean()\n",
    "        thickness_values.append(thickness_value.item())\n",
    "\n",
    "    return sum(thickness_values) / len(thickness_values)\n",
    "\n",
    "# def boundary_complexity(embeddings, D):\n",
    "#     \"\"\"\n",
    "#     Compute the boundary complexity.\n",
    "    \n",
    "#     Args:\n",
    "#     - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
    "#     - D (int): Dimensionality of the embeddings.\n",
    "    \n",
    "#     Returns:\n",
    "#     - complexity (float): The boundary complexity.\n",
    "#     \"\"\"\n",
    "#     # Compute the covariance matrix of the embeddings\n",
    "#     embeddings=torch.cat(embeddings,dim=0)\n",
    "#     covariance_matrix = torch.cov(embeddings.T)\n",
    "    \n",
    "#     # Compute the eigenvalues of the covariance matrix\n",
    "#     eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
    "#     print(eigenvalues)\n",
    "    \n",
    "#     # Normalize the eigenvalues\n",
    "#     eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
    "#     print(eigenvalues_normalized)\n",
    "    \n",
    "#     # Compute the entropy of the normalized eigenvalues\n",
    "#     entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + 1e-7))\n",
    "#     print(entropy)\n",
    "    \n",
    "#     # Normalize the entropy by dividing it by log(D)\n",
    "#     complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
    "    \n",
    "#     return complexity.item()\n",
    "def boundary_complexity(embeddings, D, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Compute the boundary complexity.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
    "    - D (int): Dimensionality of the embeddings.\n",
    "    - epsilon (float): Small value added to eigenvalues to prevent log(0).\n",
    "    \n",
    "    Returns:\n",
    "    - complexity (float): The boundary complexity.\n",
    "    \"\"\"\n",
    "    # Flatten and concatenate embeddings\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    \n",
    "    # Compute the covariance matrix of the embeddings\n",
    "    covariance_matrix = torch.cov(embeddings.T)\n",
    "    \n",
    "    # Add a small value to the diagonal for regularization\n",
    "    covariance_matrix += epsilon * torch.eye(covariance_matrix.size(0))\n",
    "    \n",
    "    # Compute the eigenvalues of the covariance matrix\n",
    "    eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
    "    \n",
    "    # Clamp eigenvalues to avoid very small negative values due to numerical errors\n",
    "    eigenvalues = torch.clamp(eigenvalues, min=epsilon)\n",
    "    \n",
    "    # Normalize the eigenvalues\n",
    "    eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
    "    \n",
    "    # Compute the entropy of the normalized eigenvalues\n",
    "    entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + epsilon))\n",
    "    \n",
    "    # Normalize the entropy by dividing it by log(D)\n",
    "    complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
    "    \n",
    "    return complexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la= 0.3\n",
    "accuracybound=[]\n",
    "stdbound=[]\n",
    "lalist=[]\n",
    "count=100\n",
    "while(la<=0.7):\n",
    "    ratio= la/(1-la)\n",
    "\n",
    "    \n",
    "\n",
    "    #print(\"Label of new graph is\",torch.argmax(new_graph[1].y,dim=-1))\n",
    "    # label=torch.argmax(boundary_graph[1].y,dim=-1)\n",
    "    # print(label)\n",
    "    #from torch_geometric.utils import to_networkx\n",
    "    \n",
    "    boundaryaccuracy=[]\n",
    "    for numexp in range(100):\n",
    "        min=1\n",
    "        boundary_graph = two_graphons_mixup(two_graphons, la=la, num_sample=90)\n",
    "        #boundary_graph=list(originaldataset)+boundary_graph\n",
    "        boundary_graph=assign_same_features_to_data(boundary_graph,Alignedfeatures[0])\n",
    "        for data in boundary_graph:\n",
    "            # num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            # data.x= torch.ones(num_nodes,1)\n",
    "            emb,out=model(data.x,data.edge_index,None)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if( abs(problities[0][0]-0.5)<min):\n",
    "                #print(\"if\")\n",
    "                min= abs(problities[0][0]-0.5)\n",
    "                boundaryprobs = problities[0][0]\n",
    " \n",
    "                bestdata=data\n",
    "        #print(\"boundaryaccuracy being appended is\",boundaryprobs )\n",
    "        boundaryaccuracy.append(boundaryprobs)\n",
    "    # examplegraph=to_networkx(bestdata,to_undirected=True)\n",
    "    # examplegraph.remove_edges_from(nx.selfloop_edges(examplegraph))\n",
    "    # plt.figure(count)\n",
    "    # nx.draw_networkx(examplegraph, node_size=150, node_color='red',with_labels=False)\n",
    "    boundaryaccuracy=torch.stack(boundaryaccuracy)\n",
    "    accuracybound.append(boundaryaccuracy.mean(dim=0))\n",
    "    print(\"Mean is\",boundaryaccuracy.mean(dim=0))\n",
    "    print(\"Std is\",boundaryaccuracy.std(dim=0))\n",
    "    stdbound.append(boundaryaccuracy.std(dim=0))\n",
    "    lalist.append(la)\n",
    "    la=la+0.05\n",
    "    count=count+1\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(accuracybound)\n",
    "# print(stdbound)\n",
    "#stdbound2=[torch.zeros_like(stdbound[i]) for i in range(len(stdbound))]\n",
    "print(accuracybound)\n",
    "print(stdbound)\n",
    "#stdbound1=[stdbound[i]/10 for i in range(len(stdbound))]\n",
    "plot_mean_with_error(accuracybound,stdbound,lalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_graph=[]\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "new_graph = two_graphons_mixup(two_graphons, la=0.5, num_sample=100)\n",
    "print(\"Label of new graph is\",new_graph[0].y)\n",
    "dataset3=list(originaldataset)\n",
    "\n",
    "newlist=list(originaldataset)+new_graph\n",
    "# print(len(newlist))\n",
    "# print(newlist[-1].x)\n",
    "newlist=prepare_dataset_x(newlist)\n",
    "explainergraph=newlist[len(dataset3):]\n",
    "count=0\n",
    "# print(explainergraph[0].x)\n",
    "#print(new_graph[0].x,new_graph[0].y,new_graph[0].edge_index)\n",
    "#max=0\n",
    "maxprob=0\n",
    "for data in explainergraph:\n",
    "    targetoutput=model(data.x,data.edge_index,None)\n",
    "    #print(targetoutput)\n",
    "    soft=torch.nn.Softmax(dim=1)\n",
    "    problitites=soft(targetoutput)\n",
    "    \n",
    "    #print(\"Probabilities are\", problitites[0][0])\n",
    "    if (abs(maxprob -  0.5)>abs(problitites[0][1] - 0.5)):\n",
    "        maxprob= problitites[0][1]\n",
    "        bestdata=data\n",
    "        print(problitites)\n",
    "\n",
    "    \n",
    "    targetpred = targetoutput.max(dim=1)[1]\n",
    "    #print(\"Label of explainer graph is\",targetpred)\n",
    "examplegraph=to_networkx(bestdata,to_undirected=True)\n",
    "plt.figure(1)\n",
    "nx.draw_networkx(examplegraph, node_size=20, node_color='lightblue',with_labels=False)\n",
    "#newlist=[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def convert_to_one_hot(data_list, num_classes=2):\n",
    "    \"\"\"\n",
    "    Convert the 'data.y' attribute of a list of PyTorch Geometric data objects into one-hot vectors.\n",
    "\n",
    "    Args:\n",
    "        data_list (list): A list of PyTorch Geometric data objects.\n",
    "        num_classes (int): The number of classes. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of PyTorch Geometric data objects with 'data.y' attribute converted into one-hot vectors.\n",
    "    \"\"\"\n",
    "    for data in data_list:\n",
    "        # Convert labels to one-hot encoding\n",
    "        one_hot = torch.zeros((len(data.y), num_classes))\n",
    "        one_hot.scatter_(1, data.y.view(-1, 1).long(), 1)\n",
    "        # Replace data.y with one-hot vectors\n",
    "        data.y = one_hot.float()\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import stat_graph, split_class_x_graphs, align_x_graphs,align_x_graphscorrected\n",
    "from utils import two_x_graphons_mixup, universal_svd\n",
    "classgraphs=split_class_x_graphs(dataset)\n",
    "Alignedfeatures=[]\n",
    "#print(\"Classgraphs is\", classgraphs)\n",
    "\n",
    "\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(dataset)\n",
    "resolution = int(median_num_nodes)\n",
    "print(\"resolution is\",resolution)\n",
    "graphons=[]\n",
    "#print(classgraphs)\n",
    "for label,graphs,nodes in classgraphs:\n",
    "    print(len(graphs))\n",
    "    print(\"Label is\",label)\n",
    "    print(\"graph is\",np.shape(graphs[0]))\n",
    "\n",
    "    print(\"Nodes is\",np.shape(nodes[1]))\n",
    "\n",
    "\n",
    "\n",
    "    align_graphs_list,alignx, normalized_node_degrees, max_num, min_num = align_x_graphscorrected(\n",
    "                    graphs,nodes, padding=True, N=resolution)\n",
    "    \n",
    "    #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "    print(\"Alignx is\",len(alignx))\n",
    "\n",
    "    graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "    print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "    graphons.append((label, graphon))\n",
    "    Alignedfeatures.append(alignx)\n",
    "two_graphons= [graphons[0] , graphons[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explain_loader= DataLoader(dataset2[:30], batch_size=1, shuffle=True)\n",
    "#newdataset=dataset\n",
    "\n",
    "classgraphs=split_class_x_graphs(newdataset)\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(newdataset)\n",
    "resolution = int(median_num_nodes)-10 # This parameter controls the number of nodes in the generated explanations\n",
    "mean_accuracy1=[]\n",
    "std_accuracy1=[]\n",
    "mean_accuracy2=[]\n",
    "std_accuracy2=[]\n",
    "ExplanationNodes=[]\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    #print(\"resolution is\",resolution)\n",
    "    stddataset=list(originaldataset)\n",
    "    graphons=[]\n",
    "    Alignedfeatures=[]\n",
    "    for label,graphs,nodes in classgraphs:\n",
    "        #print(\"Label is\",label)\n",
    "        #print(\"graph is\",graphs[0])\n",
    "        align_graphs_list,alignx, normalized_node_degrees, max_num, min_num = align_x_graphscorrected(\n",
    "                    graphs,nodes, padding=True, N=resolution)\n",
    "        #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "        graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "        Alignedfeatures.append(alignx)\n",
    "        #print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "        graphons.append((label, graphon))\n",
    "    #two_graphons = random.sample(graphons, 2)\n",
    "    two_graphons= [graphons[0] , graphons[1]]\n",
    "    #print(\"Label of graphon 0 is\",graphons[0][0], graphons[0])\n",
    "    explainer_graph1 = two_graphons_mixup(two_graphons, la=0.0, num_sample=10)\n",
    "    explainer_graph2 = two_graphons_mixup(two_graphons,la=1.0, num_sample=10)\n",
    "    explainer_graph1=convert_to_one_hot(explainer_graph1)\n",
    "    explainer_graph2=convert_to_one_hot(explainer_graph2)\n",
    "\n",
    "    label1=torch.argmax(explainer_graph1[0].y,dim=-1)\n",
    "\n",
    "    label2=torch.argmax(explainer_graph2[0].y,dim=-1)\n",
    "    # print(label1,explainer_graph1[0].y)\n",
    "    # print(label2,explainer_graph2[0].y)\n",
    "\n",
    "\n",
    "    explainer_graph1=assign_same_features_to_data(explainer_graph1,Alignedfeatures[1])\n",
    "    explainer_graph2=assign_same_features_to_data(explainer_graph2,Alignedfeatures[0])\n",
    "\n",
    "    # explainer_graph1=explainer_graph1[len(list(originaldataset)):]\n",
    "    # explainer_graph2=explainer_graph2[len(list(originaldataset)):]\n",
    "\n",
    "    accuracy1=[]\n",
    "    accuracy2=[]\n",
    "\n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "    for numexplanations in range(10):\n",
    "        max1=0\n",
    "        max2=0\n",
    "        for data in explainer_graph1:\n",
    "            # num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            # data.x= torch.ones(num_nodes,1)\n",
    "            emb,out=model(data.x,data.edge_index,None)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            \n",
    "            problities=soft(out)\n",
    "            print(\"Probabilities are\",problities)\n",
    "            if(max1<problities[0][label1]):\n",
    "                max1= problities[0][label1]\n",
    "           \n",
    "        for data in explainer_graph2:\n",
    "            # num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            # data.x= torch.ones(num_nodes,1)\n",
    "            emb,out=model(data.x,data.edge_index,None)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if (max2<problities[0][label2]):\n",
    "                max2= problities[0][label2]\n",
    "        accuracy1.append(max1)\n",
    "        accuracy2.append(max2)\n",
    "    accuracy1=torch.stack(accuracy1)\n",
    "    accuracy2=torch.stack(accuracy2)\n",
    "    mean1=accuracy1.mean(dim=0)\n",
    "    #print(\"Mean1 is\", mean1)\n",
    "    mean2=accuracy2.mean(dim=0)\n",
    "    std1=accuracy1.std(dim=0)\n",
    "    std2=accuracy2.std(dim=0)\n",
    "    mean_accuracy1.append(mean1)\n",
    "    mean_accuracy2.append(mean2)\n",
    "    std_accuracy1.append(std1)\n",
    "    std_accuracy2.append(std2)\n",
    "    ExplanationNodes.append(resolution)\n",
    "    resolution= resolution+1\n",
    "                \n",
    "            \n",
    "  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label1)\n",
    "print(mean_accuracy1)\n",
    "print(mean_accuracy2)\n",
    "print(std_accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_with_error(mean_accuracy1,std_accuracy1,ExplanationNodes)\n",
    "print(label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_with_error(mean_accuracy2,std_accuracy2,ExplanationNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "def plot_mean_with_error(mean, std, threshold,title=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plot mean with error bars.\n",
    "\n",
    "    Parameters:\n",
    "        mean (array_like): Array containing mean values.\n",
    "        std (array_like): Array containing standard deviation values.\n",
    "        threshold (array_like): Array containing threshold values.\n",
    "        label (str): Label for the data.\n",
    "        color (str): Color of the line.\n",
    "        numsample (int): Sample number.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If not provided, a new figure will be created.\n",
    "    \"\"\"\n",
    "    # Flatten the arrays\n",
    "    mean=torch.tensor(mean,dtype=torch.float32)\n",
    "    \n",
    "    std=torch.tensor(std,dtype=torch.float32)\n",
    "    mean = np.array(mean).flatten()\n",
    "    std = np.array(std).flatten()\n",
    "    threshold = np.array(threshold).flatten()\n",
    "    print(\"Threshold is\",threshold)\n",
    "    print(\"mean, std , threshold\", mean,std,threshold)\n",
    "    # # Select color automatically\n",
    "    # colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    # color = colors[numsample % 10]  # Cycle through colors\n",
    "\n",
    "    # Plotting\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.errorbar(threshold, mean, yerr=std, fmt='-')  # '-' for line\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel('Lambda')\n",
    "    ax.set_ylabel('Mean Class Score')\n",
    "    ax.set_title(title)\n",
    "    #ax.set_ylim(0.99975, 1.000051)\n",
    "\n",
    "# Optionally, set the number of ticks or their locations\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(5))  # Set max 5 ticks\n",
    "\n",
    "    # ax.legend(loc='lower right',fontsize='small')  # Show legend\n",
    "    # ax.grid(True)  # Add grid\n",
    "# # Create a figure outside the function\n",
    "# fig, ax = plt.subplots()\n",
    "# plot_mean_with_error(Mean1,Std1,Threshold,label='class1',numsample=1,ax=ax)\n",
    "# plot_mean_with_error(Mean2,Std2,Threshold,label='class1',numsample=2,ax=ax)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(originaldataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
