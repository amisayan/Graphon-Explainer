{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import torch_geometric as torch_geometric\n",
    "import math\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv,GINConv\n",
    "from torch.distributions import Bernoulli,Categorical\n",
    "import matplotlib.cm as cmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features=1\n",
    "count1=0\n",
    "totalnode=0\n",
    "for numdata in range(500):\n",
    "  range1=random.randint(2,6)\n",
    "  range2=random.randint(10,19)\n",
    "  m=[i for i in range(range1)]\n",
    "  n=[i for i in range(range1,range2)]\n",
    "  Cycle = nx.lollipop_graph(m,n)\n",
    "  num_nodes=nx.number_of_nodes(Cycle)\n",
    "  totalnode+=num_nodes\n",
    "\n",
    "  y=np.ones(num_nodes)\n",
    "\n",
    "  #Cycle.add_nodes_from([i in range(0,100)])\n",
    "  #p=math.ceil(random.uniform(5,8))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  data=pyg_utils.from_networkx(Cycle)\n",
    "  count1+=np.count_nonzero(y)\n",
    "\n",
    "  #print(y)\n",
    "\n",
    "  data.y=1\n",
    "  x=torch.ones(num_nodes,1)\n",
    "  x=x.float()\n",
    "\n",
    "  #print(deg.shape)\n",
    "\n",
    "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
    "  data.x=x\n",
    "  #dataset.append(data)\n",
    "nx.draw_networkx(Cycle, node_size=150, node_color='red',with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features=1\n",
    "count1=0\n",
    "totalnode=0\n",
    "for numdata in range(500):\n",
    "  num_nodes=random.randint(3,13)\n",
    "  totalnode+=num_nodes\n",
    "\n",
    "  y=np.ones(num_nodes)\n",
    "\n",
    "  #Cycle.add_nodes_from([i in range(0,100)])\n",
    "  p=math.ceil(random.uniform(2,4))\n",
    "  Cycle = nx.full_rary_tree(p,num_nodes)\n",
    "  #Cycle=nx.wheel_graph(num_nodes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  data=pyg_utils.from_networkx(Cycle)\n",
    "  count1+=np.count_nonzero(y)\n",
    "\n",
    "  #print(y)\n",
    "\n",
    "  data.y=0\n",
    "  x=torch.ones(num_nodes,1)\n",
    "  x=x.float()\n",
    "\n",
    "  #print(deg.shape)\n",
    "\n",
    "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
    "  data.x=x\n",
    "  dataset.append(data)\n",
    "nx.draw_networkx(Cycle, node_size=150, node_color='red',with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for loading saved dataset\n",
    "\n",
    "\n",
    "# Define the path to the saved dataset on your local machine\n",
    "\n",
    "\n",
    "# Define the path to the saved dataset on your local machine\n",
    "load_path_dataset = 'datasets/loltreedataset.pt'\n",
    "\n",
    "# Load the saved dataset (list of tensors)\n",
    "loaded_dataset = torch.load(load_path_dataset)\n",
    "\n",
    "\n",
    "# Load the saved dataset (list of tensors)\n",
    "loaded_dataset = torch.load(load_path_dataset)\n",
    "dataset=loaded_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_dataset = dataset[:350]\n",
    "test_dataset = dataset[:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data.x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.bn(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "class CombinedModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.encoder = GCNEncoder(hidden_channels)\n",
    "        self.classifier = LinearClassifier(input_dim=hidden_channels, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Get the embeddings from the encoder\n",
    "        embeddings = self.encoder(x, edge_index, batch)\n",
    "\n",
    "        # Get the logits from the classifier\n",
    "        logits = self.classifier(embeddings)\n",
    "\n",
    "        return embeddings, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CombinedModel(hidden_channels=64,num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Add a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            embedding,  out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "            #print(out)\n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        # Update the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print the current learning rate every epoch (optional)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}\",loss)\n",
    "        # train_acc = test(train_loader)\n",
    "        # test_acc = test(test_loader)\n",
    "        # print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 700\n",
    "\n",
    "# Call the training loop\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_path = '/model/2shapes.pt'\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model architecture\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(model, dataset, class_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided dataset, compute the confusion matrix,\n",
    "    and plot it with class names.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained GNN model\n",
    "    - dataset: List of data objects\n",
    "    - class_dict: Dictionary mapping class labels to class names, e.g., {0: 'Class A', 1: 'Class B'}\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Evaluate the model and get predictions and true labels\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            _, out = model(data.x, data.edge_index, data.batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_preds.append(pred)\n",
    "            all_labels.append(data.y)\n",
    "\n",
    "    all_preds = np.array(all_preds)#np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.array(all_labels)#np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Step 2: Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Step 3: Plot the confusion matrix\n",
    "    class_names = [class_dict[i] for i in range(len(class_dict))]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming the class labels are {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "#class_dict = {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "\n",
    "# Example dataset (assuming it's a list of data objects)\n",
    "# dataset = [...]\n",
    "\n",
    "# Call the function with the model, dataset (as a list), and class dictionary\n",
    "#plot_confusion_matrix(model, dataset, class_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " save_path='/model/2shapes.pt'\n",
    " torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict={0:'Tree',1:'Lollipop'}\n",
    "plot_confusion_matrix(model,dataset,class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=[]\n",
    "data2=[]\n",
    "latent_data1=[]\n",
    "latent_data2=[]\n",
    "for i in range(len(dataset)):\n",
    "    model.eval()\n",
    "    data=dataset[i]\n",
    "    embedding , out = model(data.x, data.edge_index, data.batch)\n",
    "    pred = out.argmax(dim=1)\n",
    "    if(pred==0):\n",
    "        data1.append(data)\n",
    "        latent_data1.append(embedding)\n",
    "    else:\n",
    "        data2.append(data)\n",
    "        latent_data2.append(embedding)\n",
    "\n",
    "print(len(data1))\n",
    "print(len(data2))\n",
    "\n",
    "print(data.batch)\n",
    "\n",
    "#latent_explanations=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "from utils import stat_graph, split_class_graphs, align_graphs\n",
    "from utils import two_graphons_mixup, universal_svd\n",
    "from graphon_estimator import universal_svd\n",
    "from models import GIN,GCN\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "logdir=''\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s: - %(message)s', datefmt='%Y-%m-%d')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tensorboard_writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_x(dataset):\n",
    "    if dataset[0].x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max( max_degree, degs[-1].max().item() )\n",
    "            data.num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "\n",
    "        if max_degree < 2000:\n",
    "            # dataset.transform = T.OneHotDegree(max_degree)\n",
    "\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = F.one_hot(degs, num_classes=max_degree+1).to(torch.float)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = ( (degs - mean) / std ).view( -1, 1 )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_onehot_y(dataset):\n",
    "\n",
    "    y_set = set()\n",
    "    for data in dataset:\n",
    "        y_set.add(int(data.y))\n",
    "    num_classes = len(y_set)\n",
    "\n",
    "    for data in dataset:\n",
    "        data.y = F.one_hot(data.y, num_classes=num_classes).to(torch.float)[0]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def mixup_cross_entropy_loss(input, target, size_average=True):\n",
    "    \"\"\"Origin: https://github.com/moskomule/mixup.pytorch\n",
    "    in PyTorch's cross entropy, targets are expected to be labels\n",
    "    so to predict probabilities this loss is needed\n",
    "    suppose q is the target and p is the input\n",
    "    loss(p, q) = -\\sum_i q_i \\log p_i\n",
    "    \"\"\"\n",
    "    assert input.size() == target.size()\n",
    "    assert isinstance(input, Variable) and isinstance(target, Variable)\n",
    "    loss = - torch.sum(input * target)\n",
    "    return loss / input.size()[0] if size_average else loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for graph in dataset:\n",
    "        graph.y=torch.tensor(graph.y)\n",
    "        graph.y = graph.y.view(-1)\n",
    "\n",
    "dataset = prepare_dataset_onehot_y(dataset)\n",
    "#dataset = prepare_dataset_x( dataset )\n",
    "num_features = dataset[0].x.shape[1]\n",
    "num_classes = dataset[0].y.shape[0] \n",
    "\n",
    "print(\"Num features\",num_features)\n",
    "print(\"num_classes\",num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#explain_loader= DataLoader(dataset2[:30], batch_size=1, shuffle=True)\n",
    "newdataset=dataset\n",
    "\n",
    "classgraphs=split_class_graphs(newdataset)\n",
    "\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(newdataset)\n",
    "print(\"Average num. of nodes is\",avg_num_nodes)\n",
    "print(\"Average num. of edges is\",avg_num_edges)\n",
    "resolution = int(median_num_nodes)\n",
    "#print(\"resolution is\",resolution)\n",
    "graphons=[]\n",
    "for label,graphs in classgraphs:\n",
    "    #print(\"Label is\",label)\n",
    "    #print(\"graph is\",graphs[0])\n",
    "    align_graphs_list, normalized_node_degrees, max_num, min_num = align_graphs(\n",
    "                    graphs, padding=True, N=resolution)\n",
    "    #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "    graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "    #print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "    graphons.append((label, graphon))\n",
    "#two_graphons = random.sample(graphons, 2)\n",
    "two_graphons= [graphons[0] , graphons[1]]\n",
    "\n",
    "plt.figure(1)\n",
    "print(graphons[0][0])\n",
    "plt.axis('off')\n",
    "plt.imshow(graphons[0][1],cmap=\"inferno\")\n",
    "plt.figure(2)\n",
    "print(graphons[1][0])\n",
    "plt.axis('off')\n",
    "plt.imshow(graphons[1][1],cmap='inferno')\n",
    "\n",
    "\n",
    "\n",
    "# print(new_graph)\n",
    "# print(ng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "new_graph = two_graphons_mixup(two_graphons, la=1.0, num_sample=3,show=True)\n",
    "print(\"Label of new graph is\",torch.argmax(new_graph[1].y,dim=-1))\n",
    "label=torch.argmax(new_graph[1].y,dim=-1)\n",
    "from torch_geometric.utils import to_networkx\n",
    "count=4\n",
    "for data in new_graph:\n",
    "    num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "    data.x= torch.ones(num_nodes,1)\n",
    "    embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "    soft=torch.nn.Softmax(dim=1)\n",
    "    problities=soft(out)\n",
    "    print(problities)\n",
    "    maxprob=0\n",
    "    if(maxprob<problities[0][label]):\n",
    "        maxprob=problities[0][label]\n",
    "        bestdata=data\n",
    "print(problities[0][label])\n",
    "examplegraph=to_networkx(bestdata,to_undirected=True)\n",
    "plt.figure(count+1)\n",
    "nx.draw_networkx(examplegraph, node_size=150, node_color='red',with_labels=False)\n",
    "count+=1    \n",
    "print(\"Label of new graph is\",new_graph[1].y)\n",
    "endtime=time.time()\n",
    "executiontime=endtime-start_time\n",
    "print(\"executiontime is\",executiontime)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la= 0.3\n",
    "accuracybound=[]\n",
    "stdbound=[]\n",
    "lalist=[]\n",
    "while(la<=0.7):\n",
    "    ratio= la/(1-la)\n",
    "\n",
    "    \n",
    "    boundary_graph = two_graphons_mixup(two_graphons, la=la, num_sample=100)\n",
    "    #print(\"Label of new graph is\",torch.argmax(new_graph[1].y,dim=-1))\n",
    "    label=torch.argmax(boundary_graph[1].y,dim=-1)\n",
    "    print(label)\n",
    "    #from torch_geometric.utils import to_networkx\n",
    "    \n",
    "    boundaryaccuracy=[]\n",
    "    for numexp in range(10):\n",
    "        min=1\n",
    "        for data in boundary_graph:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if( abs(problities[0][0]-0.5)<min):\n",
    "                #print(\"if\")\n",
    "                min= abs(problities[0][0]-0.5)\n",
    "                boundaryprobs = problities[0][0]\n",
    "        boundaryaccuracy.append(boundaryprobs)\n",
    "    boundaryaccuracy=torch.stack(boundaryaccuracy)\n",
    "    accuracybound.append(boundaryaccuracy.mean(dim=0))\n",
    "    stdbound.append(boundaryaccuracy.std(dim=0))\n",
    "    lalist.append(la)\n",
    "    la=la+0.05\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity Analysis on Accuracy by varying the number of nodes in the generated explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaryembeddings=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la= 0.4\n",
    "accuracybound=[]\n",
    "stdbound=[]\n",
    "lalist=[]\n",
    "\n",
    "while(la<=0.6):\n",
    "    ratio= la/(1-la)\n",
    "\n",
    "    \n",
    "    boundary_graph = two_graphons_mixup(two_graphons, la=la, num_sample=100)\n",
    "    #print(\"Label of new graph is\",torch.argmax(new_graph[1].y,dim=-1))\n",
    "    label=torch.argmax(boundary_graph[1].y,dim=-1)\n",
    "    print(label)\n",
    "    #from torch_geometric.utils import to_networkx\n",
    "    \n",
    "    boundaryaccuracy=[]\n",
    "    for numexp in range(50):\n",
    "        min=1\n",
    "        for data in boundary_graph:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if( abs(problities[0][0]-0.5)<min):\n",
    "                #print(\"if\")\n",
    "                min= abs(problities[0][0]-0.5)\n",
    "                boundaryprobs = problities[0][0]\n",
    "                latentboundary=embedding\n",
    "        boundaryaccuracy.append(boundaryprobs)\n",
    "        boundaryembeddings.append(latentboundary)\n",
    "    boundaryaccuracy=torch.stack(boundaryaccuracy)\n",
    "    accuracybound.append(boundaryaccuracy.mean(dim=0))\n",
    "    stdbound.append(boundaryaccuracy.std(dim=0))\n",
    "    lalist.append(la)\n",
    "    la=la+0.05\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cat(boundaryembeddings,dim=0).size())\n",
    "print(torch.cat(latent_data1,dim=0).size())\n",
    "print(len(boundaryembeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracybound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin=boundary_margin(latent_data1[:len(boundaryembeddings)],boundaryembeddings)\n",
    "print(margin)\n",
    "classifier=model.classifier\n",
    "\n",
    "thickness=boundary_thickness(latent_data1[:len(boundaryembeddings)], boundaryembeddings,classifier,0,1)\n",
    "print(thickness)\n",
    "complexity=boundary_complexity(boundaryembeddings,64)\n",
    "print(complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing boundary metrics\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def boundary_margin(embeddings_c1, embeddings_c2):\n",
    "    \"\"\"\n",
    "    Compute the boundary margin.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings_c1 (torch.Tensor): Embeddings of class c1 graphs.\n",
    "    - embeddings_c2 (torch.Tensor): Embeddings of boundary graphs between class c1 and c2.\n",
    "    \n",
    "    Returns:\n",
    "    - margin (float): The boundary margin.\n",
    "\n",
    "    \"\"\"\n",
    "    embeddings_c1=torch.cat(embeddings_c1,dim=0)\n",
    "    embeddings_c2=torch.cat(embeddings_c2,dim=0)\n",
    "    distances = torch.norm(embeddings_c1 - embeddings_c2, dim=1)\n",
    "    margin = torch.min(distances).item()\n",
    "    return margin\n",
    "\n",
    "def boundary_thickness(embeddings_c1, embeddings_c1_c2, model, c1, c2, gamma=0.75, num_points=100):\n",
    "    thickness_values = []\n",
    "\n",
    "    for emb_c1, emb_c1_c2 in zip(embeddings_c1, embeddings_c1_c2):\n",
    "        t_values = torch.linspace(0, 1, num_points)\n",
    "        h_t = (1 - t_values).unsqueeze(1) * emb_c1 + t_values.unsqueeze(1) * emb_c1_c2\n",
    "        print(model(h_t).size())\n",
    "\n",
    "        # Compute the logits\n",
    "        logits_h_t = model(h_t)  # Assuming `model` is your classifier\n",
    "        probs_h_t = F.softmax(logits_h_t, dim=1)\n",
    "\n",
    "        # Compute the integrand\n",
    "        integrand = (gamma > (probs_h_t[:, c1] - probs_h_t[:, c2])).float()\n",
    "\n",
    "        # Approximate the integral using the trapezoidal rule\n",
    "        integral = torch.trapz(integrand, t_values)\n",
    "\n",
    "        # Compute the thickness value\n",
    "        thickness_value = (emb_c1 - emb_c1_c2).norm() * integral.mean()\n",
    "        thickness_values.append(thickness_value.item())\n",
    "\n",
    "    return sum(thickness_values) / len(thickness_values)\n",
    "\n",
    "# def boundary_complexity(embeddings, D):\n",
    "#     \"\"\"\n",
    "#     Compute the boundary complexity.\n",
    "    \n",
    "#     Args:\n",
    "#     - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
    "#     - D (int): Dimensionality of the embeddings.\n",
    "    \n",
    "#     Returns:\n",
    "#     - complexity (float): The boundary complexity.\n",
    "#     \"\"\"\n",
    "#     # Compute the covariance matrix of the embeddings\n",
    "#     embeddings=torch.cat(embeddings,dim=0)\n",
    "#     covariance_matrix = torch.cov(embeddings.T)\n",
    "    \n",
    "#     # Compute the eigenvalues of the covariance matrix\n",
    "#     eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
    "#     print(eigenvalues)\n",
    "    \n",
    "#     # Normalize the eigenvalues\n",
    "#     eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
    "#     print(eigenvalues_normalized)\n",
    "    \n",
    "#     # Compute the entropy of the normalized eigenvalues\n",
    "#     entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + 1e-7))\n",
    "#     print(entropy)\n",
    "    \n",
    "#     # Normalize the entropy by dividing it by log(D)\n",
    "#     complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
    "    \n",
    "#     return complexity.item()\n",
    "def boundary_complexity(embeddings, D, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Compute the boundary complexity.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
    "    - D (int): Dimensionality of the embeddings.\n",
    "    - epsilon (float): Small value added to eigenvalues to prevent log(0).\n",
    "    \n",
    "    Returns:\n",
    "    - complexity (float): The boundary complexity.\n",
    "    \"\"\"\n",
    "    # Flatten and concatenate embeddings\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    \n",
    "    # Compute the covariance matrix of the embeddings\n",
    "    covariance_matrix = torch.cov(embeddings.T)\n",
    "    \n",
    "    # Add a small value to the diagonal for regularization\n",
    "    covariance_matrix += epsilon * torch.eye(covariance_matrix.size(0))\n",
    "    \n",
    "    # Compute the eigenvalues of the covariance matrix\n",
    "    eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
    "    \n",
    "    # Clamp eigenvalues to avoid very small negative values due to numerical errors\n",
    "    eigenvalues = torch.clamp(eigenvalues, min=epsilon)\n",
    "    \n",
    "    # Normalize the eigenvalues\n",
    "    eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
    "    \n",
    "    # Compute the entropy of the normalized eigenvalues\n",
    "    entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + epsilon))\n",
    "    \n",
    "    # Normalize the entropy by dividing it by log(D)\n",
    "    complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
    "    \n",
    "    return complexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explain_loader= DataLoader(dataset2[:30], batch_size=1, shuffle=True)\n",
    "newdataset=dataset\n",
    "\n",
    "classgraphs=split_class_graphs(newdataset)\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(newdataset)\n",
    "resolution = int(median_num_nodes)-4 # This parameter controls the number of nodes in the generated explanations\n",
    "mean_accuracy1=[]\n",
    "std_accuracy1=[]\n",
    "mean_accuracy2=[]\n",
    "std_accuracy2=[]\n",
    "ExplanationNodes=[]\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    #print(\"resolution is\",resolution)\n",
    "    graphons=[]\n",
    "    for label,graphs in classgraphs:\n",
    "        #print(\"Label is\",label)\n",
    "        #print(\"graph is\",graphs[0])\n",
    "        align_graphs_list, normalized_node_degrees, max_num, min_num = align_graphs(\n",
    "                        graphs, padding=True, N=resolution)\n",
    "        #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "        graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "        #print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "        graphons.append((label, graphon))\n",
    "    #two_graphons = random.sample(graphons, 2)\n",
    "    two_graphons= [graphons[0] , graphons[1]]\n",
    "    explainer_graph1 = two_graphons_mixup(two_graphons, la=0.0, num_sample=3)\n",
    "    explainer_graph2 = two_graphons_mixup(two_graphons,la=1.0, num_sample=3)\n",
    "    label1=torch.argmax(explainer_graph1[0].y,dim=-1)\n",
    "    label2=torch.argmax(explainer_graph2[0].y,dim=-1)\n",
    "    accuracy1=[]\n",
    "    accuracy2=[]\n",
    "\n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "    for numexplanations in range(100):\n",
    "        max1=0\n",
    "        max2=0\n",
    "        for data in explainer_graph1:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            \n",
    "            problities=soft(out)\n",
    "            if(max1<problities[0][label1]):\n",
    "                max1= problities[0][label1]\n",
    "           \n",
    "        for data in explainer_graph2:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if (max2<problities[0][label2]):\n",
    "                max2= problities[0][label2]\n",
    "        accuracy1.append(max1)\n",
    "        accuracy2.append(max2)\n",
    "    accuracy1=torch.stack(accuracy1)\n",
    "    accuracy2=torch.stack(accuracy2)\n",
    "    mean1=accuracy1.mean(dim=0)\n",
    "    mean2=accuracy2.mean(dim=0)\n",
    "    std1=accuracy1.std(dim=0)\n",
    "    std2=accuracy2.std(dim=0)\n",
    "    mean_accuracy1.append(mean1)\n",
    "    mean_accuracy2.append(mean2)\n",
    "    std_accuracy1.append(std1)\n",
    "    std_accuracy2.append(std2)\n",
    "    ExplanationNodes.append(resolution)\n",
    "    resolution= resolution+1\n",
    "                \n",
    "            \n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mean_with_error(mean, std, threshold,title=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plot mean with error bars.\n",
    "\n",
    "    Parameters:\n",
    "        mean (array_like): Array containing mean values.\n",
    "        std (array_like): Array containing standard deviation values.\n",
    "        threshold (array_like): Array containing threshold values.\n",
    "        label (str): Label for the data.\n",
    "        color (str): Color of the line.\n",
    "        numsample (int): Sample number.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If not provided, a new figure will be created.\n",
    "    \"\"\"\n",
    "    # Flatten the arrays\n",
    "    mean=torch.tensor(mean,dtype=torch.float32)\n",
    "    std=torch.tensor(std,dtype=torch.float32)\n",
    "    mean = np.array(mean).flatten()\n",
    "    std = np.array(std).flatten()\n",
    "    threshold = np.array(threshold).flatten()\n",
    "    # # Select color automatically\n",
    "    # colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    # coways an appropriate one to use. If the explainer method fully achieves its aims and an explanation graph it generates is very accurate, then the fake motif might actually be a valid one, and hence not \"mis\"classified? if ax is None:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.errorbar(threshold, mean, yerr=std, fmt='-')  # '-' for line\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel('Lambda')\n",
    "    ax.set_ylabel('Mean Class Score')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # ax.legend(loc='lower right',fontsize='small')  # Show legend\n",
    "    # ax.grid(True)  # Add grid\n",
    "# # Create a figure outside the function\n",
    "# fig, ax = plt.subplots()\n",
    "# plot_mean_with_error(Mean1,Std1,Threshold,label='class1',numsample=1,ax=ax)\n",
    "# plot_mean_with_error(Mean2,Std2,Threshold,label='class1',numsample=2,ax=ax)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_with_error(mean_accuracy1,std_accuracy1,ExplanationNodes)\n",
    "print(label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_with_error(mean_accuracy2,std_accuracy2,ExplanationNodes)\n",
    "print(label2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity Analysis by varying the lambda parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_with_error(accuracybound,stdbound,lalist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
