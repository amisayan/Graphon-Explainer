{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "from utils import stat_graph, split_class_graphs, align_graphs\n",
    "from utils import two_graphons_mixup, universal_svd\n",
    "from graphon_estimator import universal_svd\n",
    "from models import GIN,GCN\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "logdir=''\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s: - %(message)s', datefmt='%Y-%m-%d')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tensorboard_writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_x(dataset):\n",
    "    if dataset[0].x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max( max_degree, degs[-1].max().item() )\n",
    "            data.num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "\n",
    "        if max_degree < 2000:\n",
    "            # dataset.transform = T.OneHotDegree(max_degree)\n",
    "\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = F.one_hot(degs, num_classes=max_degree+1).to(torch.float)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            for data in dataset:\n",
    "                degs = degree(data.edge_index[0], dtype=torch.long)\n",
    "                data.x = ( (degs - mean) / std ).view( -1, 1 )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_onehot_y(dataset):\n",
    "\n",
    "    y_set = set()\n",
    "    for data in dataset:\n",
    "        y_set.add(int(data.y))\n",
    "    num_classes = len(y_set)\n",
    "\n",
    "    for data in dataset:\n",
    "        data.y = F.one_hot(data.y, num_classes=num_classes).to(torch.float)[0]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def mixup_cross_entropy_loss(input, target, size_average=True):\n",
    "    \"\"\"Origin: https://github.com/moskomule/mixup.pytorch\n",
    "    in PyTorch's cross entropy, targets are expected to be labels\n",
    "    so to predict probabilities this loss is needed\n",
    "    suppose q is the target and p is the input\n",
    "    loss(p, q) = -\\sum_i q_i \\log p_i\n",
    "    \"\"\"\n",
    "    assert input.size() == target.size()\n",
    "    assert isinstance(input, Variable) and isinstance(target, Variable)\n",
    "    loss = - torch.sum(input * target)\n",
    "    return loss / input.size()[0] if size_average else loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    graph_all = 0\n",
    "    for data in train_loader:\n",
    "        # print( \"data.y\", data.y )\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        y = data.y.view(-1, num_classes)\n",
    "        loss = mixup_cross_entropy_loss(output, y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        graph_all += data.num_graphs\n",
    "        optimizer.step()\n",
    "    loss = loss_all / graph_all\n",
    "    return model, loss\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        y = data.y.view(-1, num_classes)\n",
    "        loss += mixup_cross_entropy_loss(output, y).item() * data.num_graphs\n",
    "        y = y.max(dim=1)[1]\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += data.num_graphs\n",
    "    acc = correct / total\n",
    "    loss = loss / total\n",
    "    return acc, loss\n",
    "\n",
    "originaldataset=TUDataset(root=\"/data\",name='IMDB-BINARY')\n",
    "dataset=list(originaldataset)\n",
    "random.shuffle(dataset)\n",
    "for graph in dataset:\n",
    "        graph.y = graph.y.view(-1)\n",
    "\n",
    "#dataset = prepare_dataset_onehot_y(dataset)\n",
    "dataset = prepare_dataset_x( dataset )\n",
    "train_nums = int(len(dataset) * 0.8)\n",
    "train_val_nums = int(len(dataset) * 0.9)\n",
    "\n",
    "train_dataset = dataset[:train_nums]\n",
    "val_dataset = dataset[train_nums:train_val_nums]\n",
    "test_dataset = dataset[train_val_nums:]\n",
    "batch_size=32\n",
    "learning_rate=0.01\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "num_features = dataset[0].x.shape[1]\n",
    "num_classes = dataset[0].y.shape[0] \n",
    "\n",
    "print(\"Num features\",num_features)\n",
    "print(\"num_classes\",num_classes)\n",
    "# model = GCN(num_features=num_features, num_classes=num_classes, num_hidden=64)\n",
    "    \n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "# scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(12345)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_dataset = dataset[:800]\n",
    "test_dataset = dataset[len(train_dataset):]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data.x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, inputdim,hidden_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(inputdim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.bn(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "class CombinedModel(torch.nn.Module):\n",
    "    def __init__(self,inputdim, hidden_channels, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.encoder = GCNEncoder(inputdim,hidden_channels)\n",
    "        self.classifier = LinearClassifier(input_dim=hidden_channels, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Get the embeddings from the encoder\n",
    "        embeddings = self.encoder(x, edge_index, batch)\n",
    "\n",
    "        # Get the logits from the classifier\n",
    "        logits = self.classifier(embeddings)\n",
    "\n",
    "        return embeddings, logits\n",
    "inputdim=num_features\n",
    "model=CombinedModel(inputdim, hidden_channels=64,num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# inputdim=num_features\n",
    "# model=CombinedModel(inputdim, hidden_channels=64,num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Add a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for epoch in tqdm(range(300)):\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            #print(data.x)\n",
    "            embedding,  out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "            #y = data.y.view(-1, 2)\n",
    "            loss = criterion(out, data.y)\n",
    "            #print(out)\n",
    "            \n",
    "            #loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        # Update the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print the current learning rate every epoch (optional)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}\",loss)\n",
    "        # train_acc = test(train_loader)\n",
    "        # test_acc = test(test_loader)\n",
    "        # print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 300\n",
    "\n",
    "# Call the training loop\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,dataset):\n",
    "    model.eval()\n",
    "    acc=0\n",
    "    for data in dataset:\n",
    "        data.to(device)\n",
    "        model.to(device)\n",
    "        emb,out=model(data.x,data.edge_index,data.batch)\n",
    "        #data.y=data.y.cpu().numpy()\n",
    "        #out=out.cpu().numpy()\n",
    "        print(data.y,out.argmax(dim=1))\n",
    "        if(out.argmax(dim=1)==data.y):\n",
    "            acc+=1\n",
    "    return acc/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=test(model,test_dataset)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " save_path='/model/imdbbinary.pth'\n",
    " torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = '/model/imdbbinary.pth'\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model architecture\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(model, dataset, class_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided dataset, compute the confusion matrix,\n",
    "    and plot it with class names.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained GNN model\n",
    "    - dataset: List of data objects\n",
    "    - class_dict: Dictionary mapping class labels to class names, e.g., {0: 'Class A', 1: 'Class B'}\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Evaluate the model and get predictions and true labels\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            print(\"data.y of newdataset is\",data.y)\n",
    "            _, out = model(data.x, data.edge_index, data.batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Step 2: Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Step 3: Plot the confusion matrix\n",
    "    class_names = [class_dict[i] for i in range(len(class_dict))]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming the class labels are {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "#class_dict = {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
    "\n",
    "# Example dataset (assuming it's a list of data objects)\n",
    "# dataset = [...]\n",
    "\n",
    "# Call the function with the model, dataset (as a list), and class dictionary\n",
    "#plot_confusion_matrix(model, dataset, class_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_class_dict={0:'Action',1:'Romance'}\n",
    "reddit_class_dict={0:'Question-Answer',1:'Discussion'}\n",
    "plot_confusion_matrix(model,dataset,class_dict=imdb_class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataset2=list(originaldataset)\n",
    "classifieraccuracy=0\n",
    "random.shuffle(dataset2)\n",
    "# for graph in dataset2:\n",
    "#      graph.y = graph.y.view(-1)\n",
    "\n",
    "# dataset = prepare_dataset_onehot_y(dataset2)\n",
    "dataset2 = prepare_dataset_x( dataset2 )\n",
    "num_features = dataset2[0].x.shape[1]\n",
    "num_classes = dataset2[0].y.shape[0]\n",
    "print(num_features)\n",
    "print(num_classes)\n",
    "#explain_loader= DataLoader(dataset2[:30], batch_size=1, shuffle=True)\n",
    "newdataset=[]\n",
    "latentdata1=[]\n",
    "latentdata2=[]\n",
    "model.to('cpu')\n",
    "for data in dataset2: \n",
    "    #data=data.to(device)\n",
    "    emb,output = model(data.x, data.edge_index,data.batch)\n",
    "    #print(\"Output is\",output)\n",
    "    #print(output)\n",
    "    #output=output.to(\"cpu\")\n",
    "\n",
    "    pred = output.argmax(dim=1)\n",
    "    if(pred==data.y):\n",
    "        classifieraccuracy+=1\n",
    "    #print(pred)\n",
    "    if (pred==0):\n",
    "        data.y=torch.zeros_like(data.y)\n",
    "        latentdata1.append(emb)\n",
    "        #newdataset.append(data)\n",
    "        \n",
    "    if (pred==1):\n",
    "        data.y=torch.ones_like(data.y)\n",
    "        latentdata2.append(emb)\n",
    "   \n",
    "    newdataset.append(data)\n",
    "print(len(latentdata2))\n",
    "\n",
    "    #print(\"pred is\",pred)\n",
    "    #y = data.y.view(-1, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata=newdataset[9]\n",
    "print(testdata.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=newdataset\n",
    "classgraphs=split_class_graphs(dataset)\n",
    "\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(dataset)\n",
    "resolution = int(median_num_nodes)\n",
    "#print(\"resolution is\",resolution)\n",
    "graphons=[]\n",
    "for label,graphs in classgraphs:\n",
    "    #print(\"Label is\",label)\n",
    "    #print(\"graph is\",graphs[0])\n",
    "    align_graphs_list, normalized_node_degrees, max_num, min_num = align_graphs(\n",
    "                    graphs, padding=True, N=resolution)\n",
    "    #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "    graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "    #print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "    graphons.append((label, graphon))\n",
    "#two_graphons = random.sample(graphons, 2)\n",
    "print(graphons)\n",
    "two_graphons= [graphons[0] , graphons[1]]\n",
    "print(graphons[0][0], graphons[1][0])\n",
    "new_graph = two_graphons_mixup(two_graphons, la=1.0, num_sample=1,show=True)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)\n",
    "# print(new_graph)\n",
    "# print(ng)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv,GINConv\n",
    "from torch.distributions import Bernoulli,Categorical\n",
    "import matplotlib.cm as cmxplt\n",
    "#print(graphons[1][1])\n",
    "maxval=0.028\n",
    "plt.figure(1)\n",
    "plt.axis('off')\n",
    "print(graphons[0][0])\n",
    "plt.imshow(graphons[0][1], cmap=\"inferno\")\n",
    "plt.figure(2)\n",
    "print(graphons[1][0])\n",
    "plt.axis('off')\n",
    "plt.imshow(graphons[1][1], cmap=\"inferno\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv,GINConv\n",
    "from torch.distributions import Bernoulli,Categorical\n",
    "import matplotlib.cm as cmxplt\n",
    "#print(graphons[1][1])\n",
    "maxval=0.028\n",
    "plt.figure(1)\n",
    "plt.axis('off')\n",
    "print(graphons[0][0])\n",
    "plt.imshow(graphons[0][1], cmap=\"inferno\", vmin=0.00001,vmax=maxval)\n",
    "plt.figure(2)\n",
    "print(graphons[1][0])\n",
    "plt.axis('off')\n",
    "plt.imshow(graphons[1][1], cmap=\"inferno\", vmin=0.00001,vmax=maxval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groundtruthset=list(originaldataset)\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "groundtruthset=list(originaldataset)\n",
    "random.shuffle(groundtruthset)\n",
    "#print(groundtruthset)\n",
    "for i in range(len(groundtruthset)):\n",
    "    data=groundtruthset[i]\n",
    "    if(data.y==1):\n",
    "        class0graph=to_networkx(data,to_undirected=True)\n",
    "        class0graph.remove_edges_from(nx.selfloop_edges(class0graph))\n",
    "        plt.figure(1)\n",
    "        nx.draw_networkx(class0graph, node_size=15, node_color='lightblue',with_labels=False)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Density=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_graph=[]\n",
    "import time\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "new_graph = two_graphons_mixup(two_graphons, la=0.0, num_sample=10)\n",
    "print(\"Label of new graph is\",new_graph[0].y)\n",
    "dataset3=list(originaldataset)\n",
    "\n",
    "newlist=list(originaldataset)+new_graph\n",
    "# print(len(newlist))\n",
    "# print(newlist[-1].x)\n",
    "newlist=prepare_dataset_x(newlist)\n",
    "explainergraph=newlist[len(dataset3):]\n",
    "count=0\n",
    "# print(explainergraph[0].x)\n",
    "#print(new_graph[0].x,new_graph[0].y,new_graph[0].edge_index)\n",
    "#max=0\n",
    "maxprob=0\n",
    "start_time=time.time()\n",
    "for data in explainergraph:\n",
    "    _,targetoutput=model(data.x,data.edge_index,None)\n",
    "    #print(targetoutput)\n",
    "    soft=torch.nn.Softmax(dim=1)\n",
    "    problitites=soft(targetoutput)\n",
    "    #print(problitites)\n",
    "    targetpred = targetoutput.max(dim=1)[1]\n",
    "    \n",
    "    #print(\"Probabilities are\", problitites[0][0])\n",
    "    if (maxprob<problitites[0][1]):\n",
    "        maxprob= problitites[0][1]\n",
    "        #print(problitites)\n",
    "        bestdata=data\n",
    "        bestprob=maxprob\n",
    "\n",
    "    \n",
    "\n",
    "        print(\"Label of explainer graph is\",targetpred)\n",
    "\n",
    "\n",
    "examplegraph=to_networkx(bestdata,to_undirected=True)\n",
    "examplegraph.remove_edges_from(nx.selfloop_edges(examplegraph))\n",
    "numnodes=examplegraph.number_of_nodes()\n",
    "numedges=examplegraph.number_of_edges()\n",
    "density=numedges/(numnodes*numnodes)\n",
    "Density.append(density)\n",
    "print(\"density is\",numedges/(numnodes*numnodes))\n",
    "print(\"best probability is\",bestprob)\n",
    "for component in list(nx.connected_components(examplegraph)):\n",
    "    if len(component)<7:\n",
    "        for node in component:\n",
    "            examplegraph.remove_node(node)\n",
    "pos = nx.spring_layout(examplegraph, scale=20.0)\n",
    "plt.figure(25)\n",
    "nx.draw_networkx(examplegraph, node_size=15, node_color='lightblue',with_labels=False)\n",
    "endtime=time.time()\n",
    "executiontime=endtime-start_time\n",
    "print(\"executiontime is\",executiontime)\n",
    "#newlist=[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meandensity=np.mean(Density)\n",
    "stddensity=np.std(Density)\n",
    "print(meandensity,stddensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaryembeddings=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la= 0.4\n",
    "accuracybound=[]\n",
    "stdbound=[]\n",
    "lalist=[]\n",
    "\n",
    "while(la<=0.6):\n",
    "    ratio= la/(1-la)\n",
    "\n",
    "    \n",
    "    boundary_graph = two_graphons_mixup(two_graphons, la=la, num_sample=500)\n",
    "    boundary_graph=list(originaldataset)+boundary_graph\n",
    "    boundary_graph=prepare_dataset_x(boundary_graph)\n",
    "    boundary_graph=boundary_graph[len(originaldataset):]\n",
    "    #print(len(boundary_graph))\n",
    "    #boundary_graph= assign_same_features_to_data(boundary_graph,Alignedfeatures[0])\n",
    "    #print(\"Label of new graph is\",torch.argmax(new_graph[1].y,dim=-1))\n",
    "    label=torch.argmax(boundary_graph[1].y,dim=-1)\n",
    "    print(label)\n",
    "    #from torch_geometric.utils import to_networkx\n",
    "    \n",
    "    boundaryaccuracy=[]\n",
    "    for numexp in range(100):\n",
    "        min1=1\n",
    "        for data in boundary_graph:\n",
    "            num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            #new_graph= assign_same_features_to_data(new_graph,Alignedfeatures[0])\n",
    "            #data.x= torch.ones(num_nodes,1)\n",
    "            embedding,out=model(data.x,data.edge_index,data.batch)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            #print(problities)\n",
    "            \n",
    "            \n",
    "            if( abs(problities[0][1]-0.5)<min1):\n",
    "                #print(\"if\")\n",
    "                min1= abs(problities[0][1]-0.5)\n",
    "                boundaryprobs = problities[0][1]\n",
    "                latentboundary=embedding\n",
    "                bestdata=data\n",
    "        #plotmutag2(bestdata)\n",
    "        #print(boundaryprobs)\n",
    "        boundaryaccuracy.append(boundaryprobs)\n",
    "        boundaryembeddings.append(latentboundary)\n",
    "    boundaryaccuracy=torch.stack(boundaryaccuracy)\n",
    "    accuracybound.append(boundaryaccuracy.mean(dim=0))\n",
    "    stdbound.append(boundaryaccuracy.std(dim=0))\n",
    "    lalist.append(la)\n",
    "    la=la+0.05\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracybound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing boundary metrics\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def boundary_margin(embeddings_c1, embeddings_c2):\n",
    "    \"\"\"\n",
    "    Compute the boundary margin.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings_c1 (torch.Tensor): Embeddings of class c1 graphs.\n",
    "    - embeddings_c2 (torch.Tensor): Embeddings of boundary graphs between class c1 and c2.\n",
    "    \n",
    "    Returns:\n",
    "    - margin (float): The boundary margin.\n",
    "\n",
    "    \"\"\"\n",
    "    embeddings_c1=torch.cat(embeddings_c1,dim=0)\n",
    "    embeddings_c2=torch.cat(embeddings_c2,dim=0)\n",
    "    distances = torch.norm(embeddings_c1 - embeddings_c2, dim=1)\n",
    "    margin = torch.min(distances).item()\n",
    "    return margin\n",
    "\n",
    "def boundary_thickness(embeddings_c1, embeddings_c1_c2, model, c1, c2, gamma=0.75, num_points=100):\n",
    "    thickness_values = []\n",
    "\n",
    "    for emb_c1, emb_c1_c2 in zip(embeddings_c1, embeddings_c1_c2):\n",
    "        t_values = torch.linspace(0, 1, num_points)\n",
    "        h_t = (1 - t_values).unsqueeze(1) * emb_c1 + t_values.unsqueeze(1) * emb_c1_c2\n",
    "        #print(model(h_t).size())\n",
    "\n",
    "        # Compute the logits\n",
    "        logits_h_t = model(h_t)  # Assuming `model` is your classifier\n",
    "        probs_h_t = F.softmax(logits_h_t, dim=1)\n",
    "\n",
    "        # Compute the integrand\n",
    "        integrand = (gamma > (probs_h_t[:, c1] - probs_h_t[:, c2])).float()\n",
    "\n",
    "        # Approximate the integral using the trapezoidal rule\n",
    "        integral = torch.trapz(integrand, t_values)\n",
    "\n",
    "        # Compute the thickness value\n",
    "        thickness_value = (emb_c1 - emb_c1_c2).norm() * integral.mean()\n",
    "        thickness_values.append(thickness_value.item())\n",
    "\n",
    "    return sum(thickness_values) / len(thickness_values)\n",
    "\n",
    "# def boundary_complexity(embeddings, D):\n",
    "#     \"\"\"\n",
    "#     Compute the boundary complexity.\n",
    "    \n",
    "#     Args:\n",
    "#     - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
    "#     - D (int): Dimensionality of the embeddings.\n",
    "    \n",
    "#     Returns:\n",
    "#     - complexity (float): The boundary complexity.\n",
    "#     \"\"\"\n",
    "#     # Compute the covariance matrix of the embeddings\n",
    "#     embeddings=torch.cat(embeddings,dim=0)\n",
    "#     covariance_matrix = torch.cov(embeddings.T)\n",
    "    \n",
    "#     # Compute the eigenvalues of the covariance matrix\n",
    "#     eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
    "#     print(eigenvalues)\n",
    "    \n",
    "#     # Normalize the eigenvalues\n",
    "#     eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
    "#     print(eigenvalues_normalized)\n",
    "    \n",
    "#     # Compute the entropy of the normalized eigenvalues\n",
    "#     entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + 1e-7))\n",
    "#     print(entropy)\n",
    "    \n",
    "#     # Normalize the entropy by dividing it by log(D)\n",
    "#     complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
    "    \n",
    "#     return complexity.item()\n",
    "def boundary_complexity(embeddings, D, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Compute the boundary complexity.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
    "    - D (int): Dimensionality of the embeddings.\n",
    "    - epsilon (float): Small value added to eigenvalues to prevent log(0).\n",
    "    \n",
    "    Returns:\n",
    "    - complexity (float): The boundary complexity.\n",
    "    \"\"\"\n",
    "    # Flatten and concatenate embeddings\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    \n",
    "    # Compute the covariance matrix of the embeddings\n",
    "    covariance_matrix = torch.cov(embeddings.T)\n",
    "    \n",
    "    # Add a small value to the diagonal for regularization\n",
    "    covariance_matrix += epsilon * torch.eye(covariance_matrix.size(0))\n",
    "    \n",
    "    # Compute the eigenvalues of the covariance matrix\n",
    "    eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
    "    \n",
    "    # Clamp eigenvalues to avoid very small negative values due to numerical errors\n",
    "    eigenvalues = torch.clamp(eigenvalues, min=epsilon)\n",
    "    \n",
    "    # Normalize the eigenvalues\n",
    "    eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
    "    \n",
    "    # Compute the entropy of the normalized eigenvalues\n",
    "    entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + epsilon))\n",
    "    \n",
    "    # Normalize the entropy by dividing it by log(D)\n",
    "    complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
    "    \n",
    "    return complexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latentclass=latentdata2\n",
    "margin=boundary_margin(latentclass[:len(boundaryembeddings)],boundaryembeddings)\n",
    "print(\"margin is\",margin)\n",
    "classifier=model.classifier\n",
    "\n",
    "thickness=boundary_thickness(latentclass[:len(boundaryembeddings)] ,boundaryembeddings,classifier,0,1)\n",
    "print(thickness)\n",
    "print(\"thickness is\",thickness)\n",
    "complexity=boundary_complexity(boundaryembeddings,64)\n",
    "print(\"complexity is\",complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la= 0.3\n",
    "accuracybound=[]\n",
    "stdbound=[]\n",
    "lalist=[]\n",
    "count=100\n",
    "while(la<=0.7):\n",
    "    ratio= la/(1-la)\n",
    "\n",
    "    \n",
    "\n",
    "    #print(\"Label of new graph is\",torch.argmax(new_graph[1].y,dim=-1))\n",
    "    # label=torch.argmax(boundary_graph[1].y,dim=-1)\n",
    "    # print(label)\n",
    "    #from torch_geometric.utils import to_networkx\n",
    "    \n",
    "    boundaryaccuracy=[]\n",
    "    for numexp in range(10):\n",
    "        min=1\n",
    "        boundary_graph = two_graphons_mixup(two_graphons, la=la, num_sample=10)\n",
    "        boundary_graph=list(originaldataset)+boundary_graph\n",
    "        boundary_graph=prepare_dataset_x(boundary_graph)\n",
    "        for data in boundary_graph:\n",
    "            # num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            # data.x= torch.ones(num_nodes,1)\n",
    "            out=model(data.x,data.edge_index,None)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if( abs(problities[0][0]-0.5)<min):\n",
    "                #print(\"if\")\n",
    "                min= abs(problities[0][0]-0.5)\n",
    "                boundaryprobs = problities[0][0]\n",
    " \n",
    "                bestdata=data\n",
    "        print(\"boundaryaccuracy being appended is\",boundaryprobs )\n",
    "        boundaryaccuracy.append(boundaryprobs)\n",
    "    examplegraph=to_networkx(bestdata,to_undirected=True)\n",
    "    examplegraph.remove_edges_from(nx.selfloop_edges(examplegraph))\n",
    "    plt.figure(count)\n",
    "    nx.draw_networkx(examplegraph, node_size=20, node_color='lightblue',with_labels=False)\n",
    "    boundaryaccuracy=torch.stack(boundaryaccuracy)\n",
    "    accuracybound.append(boundaryaccuracy.mean(dim=0))\n",
    "    print(\"Mean is\",boundaryaccuracy.mean(dim=0))\n",
    "    print(\"Std is\",boundaryaccuracy.std(dim=0))\n",
    "    stdbound.append(boundaryaccuracy.std(dim=0))\n",
    "    lalist.append(la)\n",
    "    la=la+0.05\n",
    "    count=count+1\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "\n",
    "# ng=two_graphons_mixup(two_graphons,la=1.0,num_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(accuracybound)\n",
    "# print(stdbound)\n",
    "#stdbound2=[torch.zeros_like(stdbound[i]) for i in range(len(stdbound))]\n",
    "plot_mean_with_error(accuracybound,stdbound,lalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_graph=[]\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "new_graph = two_graphons_mixup(two_graphons, la=0.5, num_sample=100)\n",
    "print(\"Label of new graph is\",new_graph[0].y)\n",
    "dataset3=list(originaldataset)\n",
    "\n",
    "newlist=list(originaldataset)+new_graph\n",
    "# print(len(newlist))\n",
    "# print(newlist[-1].x)\n",
    "newlist=prepare_dataset_x(newlist)\n",
    "explainergraph=newlist[len(dataset3):]\n",
    "count=0\n",
    "# print(explainergraph[0].x)\n",
    "#print(new_graph[0].x,new_graph[0].y,new_graph[0].edge_index)\n",
    "#max=0\n",
    "maxprob=0\n",
    "for data in explainergraph:\n",
    "    targetoutput=model(data.x,data.edge_index,None)\n",
    "    #print(targetoutput)\n",
    "    soft=torch.nn.Softmax(dim=1)\n",
    "    problitites=soft(targetoutput)\n",
    "    \n",
    "    #print(\"Probabilities are\", problitites[0][0])\n",
    "    if (abs(maxprob -  0.5)>abs(problitites[0][1] - 0.5)):\n",
    "        maxprob= problitites[0][1]\n",
    "        bestdata=data\n",
    "        print(problitites)\n",
    "\n",
    "    \n",
    "    targetpred = targetoutput.max(dim=1)[1]\n",
    "    #print(\"Label of explainer graph is\",targetpred)\n",
    "examplegraph=to_networkx(bestdata,to_undirected=True)\n",
    "plt.figure(1)\n",
    "nx.draw_networkx(examplegraph, node_size=20, node_color='lightblue',with_labels=False)\n",
    "#newlist=[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def convert_to_one_hot(data_list, num_classes=2):\n",
    "    \"\"\"\n",
    "    Convert the 'data.y' attribute of a list of PyTorch Geometric data objects into one-hot vectors.\n",
    "\n",
    "    Args:\n",
    "        data_list (list): A list of PyTorch Geometric data objects.\n",
    "        num_classes (int): The number of classes. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of PyTorch Geometric data objects with 'data.y' attribute converted into one-hot vectors.\n",
    "    \"\"\"\n",
    "    for data in data_list:\n",
    "        # Convert labels to one-hot encoding\n",
    "        one_hot = torch.zeros((len(data.y), num_classes))\n",
    "        one_hot.scatter_(1, data.y.view(-1, 1).long(), 1)\n",
    "        # Replace data.y with one-hot vectors\n",
    "        data.y = one_hot.float()\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explain_loader= DataLoader(dataset2[:30], batch_size=1, shuffle=True)\n",
    "#newdataset=dataset\n",
    "\n",
    "classgraphs=split_class_graphs(newdataset)\n",
    "avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(newdataset)\n",
    "resolution = int(median_num_nodes)-10 # This parameter controls the number of nodes in the generated explanations\n",
    "mean_accuracy1=[]\n",
    "std_accuracy1=[]\n",
    "mean_accuracy2=[]\n",
    "std_accuracy2=[]\n",
    "ExplanationNodes=[]\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    #print(\"resolution is\",resolution)\n",
    "    stddataset=list(originaldataset)\n",
    "    graphons=[]\n",
    "    for label,graphs in classgraphs:\n",
    "        #print(\"Label is\",label)\n",
    "        #print(\"graph is\",graphs[0])\n",
    "        align_graphs_list, normalized_node_degrees, max_num, min_num = align_graphs(\n",
    "                        graphs, padding=True, N=resolution)\n",
    "        #print(\"Aligned adj\",align_graphs_list[8].shape,align_graphs_list[56].shape)\n",
    "        graphon = universal_svd(align_graphs_list, threshold=0.2)\n",
    "        #print(\"Graphon is \",graphon.shape)\n",
    "\n",
    "        graphons.append((label, graphon))\n",
    "    #two_graphons = random.sample(graphons, 2)\n",
    "    two_graphons= [graphons[0] , graphons[1]]\n",
    "    #print(\"Label of graphon 0 is\",graphons[0][0], graphons[0])\n",
    "    explainer_graph1 = two_graphons_mixup(two_graphons, la=0.0, num_sample=250)\n",
    "    explainer_graph2 = two_graphons_mixup(two_graphons,la=1.0, num_sample=5)\n",
    "    explainer_graph1=convert_to_one_hot(explainer_graph1)\n",
    "    explainer_graph2=convert_to_one_hot(explainer_graph2)\n",
    "\n",
    "    label1=torch.argmax(explainer_graph1[0].y,dim=-1)\n",
    "\n",
    "    label2=torch.argmax(explainer_graph2[0].y,dim=-1)\n",
    "    # print(label1,explainer_graph1[0].y)\n",
    "    # print(label2,explainer_graph2[0].y)\n",
    "\n",
    "\n",
    "    explainer_graph1=list(originaldataset)+explainer_graph1\n",
    "    explainer_graph2=list(originaldataset)+explainer_graph2\n",
    "    explainer_graph1=prepare_dataset_x(explainer_graph1)\n",
    "    explainer_graph2=prepare_dataset_x(explainer_graph2)\n",
    "\n",
    "    explainer_graph1=explainer_graph1[len(list(originaldataset)):]\n",
    "    explainer_graph2=explainer_graph2[len(list(originaldataset)):]\n",
    "\n",
    "    accuracy1=[]\n",
    "    accuracy2=[]\n",
    "\n",
    "    #print(\"Label of new graph is\",new_graph[1].y)\n",
    "\n",
    "\n",
    "    for numexplanations in range(10):\n",
    "        max1=0\n",
    "        max2=0\n",
    "        for data in explainer_graph1:\n",
    "            # num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            # data.x= torch.ones(num_nodes,1)\n",
    "            out=model(data.x,data.edge_index,None)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            \n",
    "            problities=soft(out)\n",
    "            print(\"Probabilities are\",problities)\n",
    "            if(max1<problities[0][label1]):\n",
    "                max1= problities[0][label1]\n",
    "           \n",
    "        for data in explainer_graph2:\n",
    "            # num_nodes = int( torch.max(data.edge_index) ) + 1\n",
    "            # data.x= torch.ones(num_nodes,1)\n",
    "            out=model(data.x,data.edge_index,None)\n",
    "            soft=torch.nn.Softmax(dim=1)\n",
    "            problities=soft(out)\n",
    "            if (max2<problities[0][label2]):\n",
    "                max2= problities[0][label2]\n",
    "        accuracy1.append(max1)\n",
    "        accuracy2.append(max2)\n",
    "    accuracy1=torch.stack(accuracy1)\n",
    "    accuracy2=torch.stack(accuracy2)\n",
    "    mean1=accuracy1.mean(dim=0)\n",
    "    #print(\"Mean1 is\", mean1)\n",
    "    mean2=accuracy2.mean(dim=0)\n",
    "    std1=accuracy1.std(dim=0)\n",
    "    std2=accuracy2.std(dim=0)\n",
    "    mean_accuracy1.append(mean1)\n",
    "    mean_accuracy2.append(mean2)\n",
    "    std_accuracy1.append(std1)\n",
    "    std_accuracy2.append(std2)\n",
    "    ExplanationNodes.append(resolution)\n",
    "    resolution= resolution+1\n",
    "                \n",
    "            \n",
    "  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label1)\n",
    "print(mean_accuracy1)\n",
    "print(mean_accuracy2)\n",
    "print(std_accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_with_error(mean_accuracy1,std_accuracy1,ExplanationNodes)\n",
    "print(label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_with_error(mean_accuracy2,std_accuracy2,ExplanationNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mean_with_error(mean, std, threshold,title=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plot mean with error bars.\n",
    "\n",
    "    Parameters:\n",
    "        mean (array_like): Array containing mean values.\n",
    "        std (array_like): Array containing standard deviation values.\n",
    "        threshold (array_like): Array containing threshold values.\n",
    "        label (str): Label for the data.\n",
    "        color (str): Color of the line.\n",
    "        numsample (int): Sample number.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If not provided, a new figure will be created.\n",
    "    \"\"\"\n",
    "    # Flatten the arrays\n",
    "    mean=torch.tensor(mean,dtype=torch.float32)\n",
    "    \n",
    "    std=torch.tensor(std,dtype=torch.float32)\n",
    "    mean = np.array(mean).flatten()\n",
    "    std = np.array(std).flatten()\n",
    "    threshold = np.array(threshold).flatten()\n",
    "    print(\"mean, std , threshold\", mean,std,threshold)\n",
    "    # # Select color automatically\n",
    "    # colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    # color = colors[numsample % 10]  # Cycle through colors\n",
    "\n",
    "    # Plotting\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.errorbar(threshold, mean, yerr=std, fmt='-')  # '-' for line\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel('Lambda')\n",
    "    ax.set_ylabel('Mean Class Score')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # ax.legend(loc='lower right',fontsize='small')  # Show legend\n",
    "    # ax.grid(True)  # Add grid\n",
    "# # Create a figure outside the function\n",
    "# fig, ax = plt.subplots()\n",
    "# plot_mean_with_error(Mean1,Std1,Threshold,label='class1',numsample=1,ax=ax)\n",
    "# plot_mean_with_error(Mean2,Std2,Threshold,label='class1',numsample=2,ax=ax)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(originaldataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
